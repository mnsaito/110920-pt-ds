{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:53:53.131534Z", "start_time": "2021-04-13T20:53:53.097749Z"}}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.datasets import load_breast_cancer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# K-Nearest Neighbors\n", "\n", "![wilson](img/wilson.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A breakdown of the algorithm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Fit**\n", "\n", "- KNN is considered a lazy learning algorithm. This is because, when we fit a KNN model, all the model does is store the training data as a model attribute.\n", "\n", "**Predict**\n", "\n", "When we pass features to a model in order to make predictions, this is what happens:\n", "\n", "1. We loop over every row in the training data\n", "2. We calculate the distance between the row of features from the training data and the row of features in the data meant for prediction. \n", "3. We store the distance for every row in the training data\n", "4. Sort the array of distances to find the `k` number of rows in the training data that are closest to the prediction data.\n", "5. We then count how many times each class appears in the `k` number of training data rows. The class that appears the most becomes the predictions. \n", "\n", "> **Note:** It is important to point out that all of the true functionality of KNN takes place during the prediction phase. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](img/visual-explainer.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Distance Metrics**\n", "\n", "Euclidean distance:\n", "\n", "`sqrt(sum((x - y)^2))`\n", "\n", "Manhattan Distance:\n", "\n", "`sum(|x - y|)`\n", "\n", "There a number of distance metrics that can be used with KNN. When using Sklearn, the available metrics can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric).\n", "\n", "For a more detailed breakdown of each metric, check out [this](https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.html) article."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's code out both of these metrics!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def euclidean(vector_1, vector_2):\n", "    \"\"\"\n", "    Find the euclidean distance between two vectors:\n", "             \n", "             `sqrt(sum((vector_1 - vector_2)^2))`\n", "    \n", "    vector_1 = An array containing numeric data\n", "    vector_2 = An array containing numeric data\n", "    \n", "    Both vectors must have the same dimensions.\n", "    \"\"\"\n", "    # Subtract vector_2 from vector_1\n", "    \n", "    # Square the difference\n", "    \n", "    # Sum the squared distance\n", "    \n", "    # Return the square root of the sum\n", "    \n", "    \n", "def manhattan(vector_1, vector_2):\n", "    \"\"\"\n", "    Find the manhattan distance between two vectors:\n", "             \n", "             `sum(|vector_1 - vector_2|)`\n", "    \n", "    vector_1 = An array containing numeric data\n", "    vector_2 = An array containing numeric data\n", "    \n", "    Both vectors must have the same dimensions.\n", "    \"\"\"\n", "    # Subtract vector_2 from vector_1\n", "    \n", "    # Find the absolute of the distance\n", "    \n", "    # Return the sum of the absolute distance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Let's test out our functions!**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:10:30.830747Z", "start_time": "2021-04-13T20:10:30.795473Z"}}, "outputs": [], "source": ["np.random.seed(2021)\n", "vector_1 = np.random.normal(size=100)\n", "vector_2 = np.random.normal(loc=3, scale=3, size=100)\n", "\n", "vector_a = np.random.exponential(1, 100)\n", "vector_b = np.random.exponential(3, 100)\n", "\n", "print(euclidean(vector_1, vector_2)) # 43.2407625261724\n", "print(manhattan(vector_1, vector_2)) # 357.6012129494707\n", "print(euclidean(vector_a, vector_b)) # 32.61377501006896\n", "print(manhattan(vector_a, vector_b)) # 198.35518667980242"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Ok, let's write code to generate predictions.**\n", "\n", "In the cell below, create two functions:\n", "\n", "1. `find_neighbors`\n", "        \n", "2. `predict`\n", "\n", "Both functions should receive 5 arguments:\n", "    \n", "1. `X_train` - The training data\n", "2. `y_train` - The labels for the training data\n", "3. `test_observation` - A single vector for which we would like to generate predictions\n", "4. `distance_function` - A function for calculating the distance between vectors\n", "5. `k` - The number of nearest neighbors we would like to consider\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def find_neighbors(X_train, y_train, test_observation, distance_function, k):\n", "    \"\"\"\n", "    Loops over every observation in the training data and calcuates the distance\n", "    between each training observation and the testing_observation vector. \n", "    ==============================================================================\n", "    X_train =           Training Features Matrix\n", "    \n", "    y_train =           Training Class Labels\n", "    \n", "    test_observation =  Vector of features for a single observation\n", "    \n", "    distance_function = Function used for calculating the distance between vectors\n", "    \n", "    k =                 The number of nearest training observations that contribute \n", "                        a vote to the predicted class     \n", "    ==============================================================================\n", "    Returns: array. k observations from y_train that have the k smallest distances.\n", "    \"\"\"\n", "    # Create an array for storing distances\n", "\n", "    # Loop over the index of training data\n", "\n", "        # Find the distance between the test_observation and\n", "        # the training observation at the given index\n", "\n", "        # Store the distance in the distance array\n", "\n", "    # Find the top k indices in the distances array \n", "    # with the smallest distance\n", "\n", "    \n", "    # Use the indices for smallest distances\n", "    # to slice y_train and return a list of \n", "    # class labels for the k nearest neighbors\n", "\n", "\n", "\n", "def predict(X_train, y_train, X_test, distance_function, k):\n", "    \"\"\"\n", "    Generates predictions for every observation in X_test\n", "    ==============================================================================\n", "    X_train =           Training Features Matrix\n", "    \n", "    y_train =           Training Class Labels\n", "    \n", "    X_test =            Matrix of observations for which prediction \n", "                        will be generated\n", "    \n", "    distance_function = Function used for calculating the distance between vectors\n", "    \n", "    k =                 The number of nearest training observations that contribute \n", "                        a vote to the predicted class\n", "    \"\"\"\n", "    # Create an array to store predictions\n", "\n", "    # Loop over the index of the testing data\n", "\n", "        # Find the k nearest neighbors class labels\n", "\n", "        # Find the most common class label\n", "\n", "        # Append the most common class label to the predictions array\n", "\n", "    # Return predictions\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's test our code!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:42:44.077405Z", "start_time": "2021-04-13T20:42:43.254139Z"}}, "outputs": [], "source": ["# Load a classification dataset\n", "data = load_breast_cancer()\n", "X = data['data']\n", "y = data['target']\n", "\n", "# Create a train test split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y)\n", "\n", "# Calculate predictions\n", "y_hat_euclidean = y_hat = predict(X_train, y_train, X_test, euclidean, 5)\n", "y_hat_manhattan = predict(X_train, y_train, X_test, manhattan, 5)\n", "\n", "print('Euclidean:', accuracy_score(y_test, y_hat_euclidean))\n", "print('Manhattan:', accuracy_score(y_test, y_hat_manhattan))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Can we improve our score?\n", "\n", "Below, we will do something called a `GridSearch` that scores the model on every combination of hyperparameters from a list of options we would like to consider."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:52:44.917101Z", "start_time": "2021-04-13T20:52:41.174534Z"}}, "outputs": [], "source": ["def score_model(X_train, y_train, X_test, y_test, distance_function, k):\n", "    preds = predict(X_train, y_train, X_test, distance_function, k)\n", "    return accuracy_score(y_test, preds)\n", "\n", "functions = {'euclidean': euclidean, 'manhattan': manhattan}\n", "\n", "ks = np.arange(1, 10, 2)\n", "scores = []\n", "for k in ks:\n", "    for function in functions:\n", "        score = score_model(X_train, y_train, X_test, y_test, functions[function], k)\n", "        scores.append({'k': k, 'distance': function, 'score': score})\n", "        \n", "best_model = sorted(scores, key=lambda x: x['score'], reverse=True)[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:52:46.120299Z", "start_time": "2021-04-13T20:52:46.086459Z"}}, "outputs": [], "source": ["best_model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementation with Sklearn"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:07:16.353218Z", "start_time": "2021-04-13T21:07:16.292048Z"}}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.model_selection import cross_val_score\n", "\n", "model = KNeighborsClassifier(n_neighbors=3, p=1)\n", "cross_val_score(model, X_train, y_train, cv=5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Running a GridSearch with Sklearn**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:11:01.833157Z", "start_time": "2021-04-13T21:11:01.606985Z"}}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "parameters = {'n_neighbors':np.arange(1,10,2), 'p':[1, 2]}\n", "\n", "model = KNeighborsClassifier()\n", "grid_search = GridSearchCV(model, parameters, cv=5)\n", "grid_search.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Accessing the best parameters from a gridsearch:**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:11:02.974479Z", "start_time": "2021-04-13T21:11:02.941415Z"}}, "outputs": [], "source": ["grid_search.best_params_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Accessing the best score from the gridsearch:**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:11:03.600444Z", "start_time": "2021-04-13T21:11:03.567692Z"}}, "outputs": [], "source": ["grid_search.best_score_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Accessing the best model from the gridsearch:**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:11:04.279052Z", "start_time": "2021-04-13T21:11:04.246130Z"}}, "outputs": [], "source": ["best_model = grid_search.best_estimator_\n", "best_model.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:11:05.217747Z", "start_time": "2021-04-13T21:11:05.178296Z"}}, "outputs": [], "source": ["best_model.score(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Some final notes about KNN**\n", "\n", "KNearest Neighbors is the second classification algorithm in our toolbelt added to our logistic regression classifier.\n", "\n", "If we remember, logistic regression is a supervised, parametric, discriminative model.\n", "\n", "KNN is a **supervised, non-parametric, discriminative, lazy-learning algorithm.**\n", "\n", "Let's break down those terms:\n", "\n", "**Supervised:** The problem is supervised because we have a `target` column.\n", "\n", "**Non Parametric:** Linear and Logistic Regression have a set number of parameters, (ie the coefficients and the and the intercept) that are tied to the shape of the data. Non Parametric models do not have a set number of parameters that are tied to the datas' shape. \n", "\n", "**Discriminative:** \"Discriminative models, also referred to as conditional models, are a class of logistical models used for classification or regression. They distinguish decision boundaries through observed data\" - [Wikipedia](https://en.wikipedia.org/wiki/Discriminative_model#:~:text=Discriminative%20models%2C%20also%20referred%20to,%2Fdead%20or%20healthy%2Fsick.)\n", "\n", "**Lazy Learner:** KNN is a lazy learner because all of the data crunching happens in the prediction phase instead of the fit phase. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["![alt text](img/K-NN_Neighborhood_Size_print.png)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 4}