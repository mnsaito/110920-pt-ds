{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Cross-Validation & Regularization"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:48.299665Z", "start_time": "2021-03-30T21:29:46.620186Z"}}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n", "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression,\\\n", "LassoCV, RidgeCV, ElasticNetCV\n", "from sklearn.model_selection import train_test_split, KFold,\\\n", "cross_val_score, cross_validate, ShuffleSplit\n", "import pandas as pd\n", "from sklearn.metrics import mean_squared_error\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One of the goals of a machine learning project is to make models which are highly predictive.\n", "If the model fails to generalize to unseen data then the model is bad."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Review of Earlier concepts\n", "\n", "### Overfitting vs Underfitting\n", "    1. Underfit models fail to capture all of the information in the data\n", "        1. Ex. Just predicting the mean leaves a lot of information on the table\n", "    2. Overfit models fit to the noise in the data and fail to generalize\n", "    3. How would we know if our model is over- or underfit?\n", "        1. Train test split\n", "        2. Look at the testing error\n", "        3. As model complexity increases so does the possibility for overfitting"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Bias-Variance Tradeoff\n", "    1. High bias\n", "        1. Systematic error in predictions\n", "        2. Bias is about the strength of assumptions the model makes\n", "        3. Underfit models tend to have high bias\n", "    2. High variance\n", "        1. The model is highly sensitive to changes in the data\n", "        2. Overfit models tend to have low bias"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Example\n", "\n", "High bias is easy to wrap one's mind around: Imagine pulling three red balls from an urn that has hundreds of balls of all colors in a uniform distribution. Then my sample is a terrible representative of the whole population. If I were to build a model by extrapolating from my sample, that model would predict that _every_ ball produced would be red! That is, this model would be incredibly biased.\n", "\n", "High variance is a little bit harder to visualize, but it's basically the \"opposite\" of this. Imagine that the population of balls in the urn is mostly red, but also that there are a few balls of other colors floating around. Now imagine that our sample comprises a few balls, none of which is red. In this case, we've essentially picked up on the \"noise\", rather than the \"signal\". If I were to build a model by extrapolating from my sample, that model would be needlessly complex. It might predict that balls drawn before noon will be orange and that balls drawn after 8pm will be green, when the reality is that a simple model that predicted 'red' for all balls would be a superior model!\n", "\n", "The important idea here is that there is a *trade-off*: If we have too few data in our sample (training set), or too few predictors, we run the risk of high *bias*, i.e. an underfit model. On the other hand, if we have too many predictors (especially ones that are collinear), we run the risk of high *variance*, i.e. an overfit model.\n", "\n", "[Here](https://en.wikipedia.org/wiki/Overfitting#/media/File:Overfitting.svg) is a nice illustration of the difficulty."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Validation\n", "\n", "So: You've split your data in two, reserving one piece for training your model and the other for testing it after it's built. That's good!\n", "\n", "But generally speaking we want to take more precautions than this. After all, we're still imagining building just one model on the training set and then crossing our fingers for its performance on the test set.\n", "\n", "Data scientists often distinguish *three* subsets of data: training, **validation**, and testing.\n", "\n", "Roughly:\n", "- Training data is for building the model;\n", "- Validation data is for *tweaking* the model;\n", "- Testing data is for evaluating the model on unseen data.\n", "\n", "This \"tweaking\" includes most of all the fine-tuning of model parameters (see below). Think of what this three-way distinction allows us to do:\n", "\n", "I can build a model on some data. Then, **before** I introduce the model to the testing data, I can introduce it to a different batch of data (the validation set). With respect to the validation data I can do things like measure error and tweak model parameters to minimize that error. Of course, I also don't want to lose sight of the error on the training data. If the model error has been minimized on the training error, then of course any changes I make to the model parameters will take me away from that minimum. But still the new information I've gained by looking at the model's performance on the validiation data is valuable. I might for example go with a kind of compromising model whose parameters produce an error that's not too big on the training data and not too big on the validation data.\n", "\n", "**Question**: What's different about this procedure from what we've described before? Aren't I just calling the test data \"validation data\" now? Is there any substantive difference?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### From Validation to Cross-Validation\n", "\n", "Since my model will \"see\" the validation data in any case, I might as well use *all* of my training data to validate my model! How do I do this?\n", "\n", "Cross-validation works like this: First I'll partition my training data into $k$-many *folds*. Then I'll train a model on $k-1$ of those folds and \"test\" it on the remaining fold. I'll do this for all possible divisions of my $k$ folds into $k-1$ training folds and a single \"testing\" fold. Since there are $k\\choose 1$$=k$-many ways of doing this, I'll be building $k$-many models!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### In Python"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:51.971811Z", "start_time": "2021-03-30T21:29:51.951191Z"}}, "outputs": [], "source": ["birds = sns.load_dataset('penguins')\n", "birds.sample(5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:52.933399Z", "start_time": "2021-03-30T21:29:52.926288Z"}}, "outputs": [], "source": ["birds.info()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:53.320691Z", "start_time": "2021-03-30T21:29:53.317062Z"}}, "outputs": [], "source": ["# For simplicity's sake we'll limit our analysis to the numeric columns.\n", "\n", "numeric = birds[['bill_length_mm', 'bill_depth_mm',\n", "                 'flipper_length_mm', 'body_mass_g']]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:54.031388Z", "start_time": "2021-03-30T21:29:54.026314Z"}}, "outputs": [], "source": ["# We'll drop the rows with null values\n", "\n", "numeric = numeric.dropna().reset_index()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initiating the `KFold()` object"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:54.714410Z", "start_time": "2021-03-30T21:29:54.711629Z"}}, "outputs": [], "source": ["folds = KFold(n_splits=10, shuffle=True, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:55.042301Z", "start_time": "2021-03-30T21:29:55.038705Z"}}, "outputs": [], "source": ["folds.split(numeric)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:55.414813Z", "start_time": "2021-03-30T21:29:55.401676Z"}}, "outputs": [], "source": ["[bird for bird in folds.split(numeric)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Suppose I want to model `body_mass_g` as a function of the other attributes."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:56.974744Z", "start_time": "2021-03-30T21:29:56.970717Z"}}, "outputs": [], "source": ["X = numeric.drop('body_mass_g', axis=1)\n", "y = numeric['body_mass_g']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll make ten models and record our evaluations of them."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:57.718561Z", "start_time": "2021-03-30T21:29:57.666381Z"}}, "outputs": [], "source": ["r2_scores = np.array([])\n", "\n", "# For each partition in our list:\n", "for train, test in folds.split(numeric):\n", "    \n", "    # We'll train a model on the nine-fold part ...\n", "    X_training = X.loc[train, :]\n", "    \n", "    # and test it on the fold left over.\n", "    X_testing = X.loc[test, :]\n", "    y_training = y.loc[train]\n", "    y_testing = y.loc[test]\n", "    lr = LinearRegression()\n", "    lr.fit(X_training, y_training)\n", "    \n", "    # We'll keep track of all of the R^2 scores.\n", "    r2_score = lr.score(X_testing, y_testing)\n", "    r2_scores = np.append(r2_scores, r2_score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:58.683316Z", "start_time": "2021-03-30T21:29:58.679629Z"}}, "outputs": [], "source": ["r2_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:59.028994Z", "start_time": "2021-03-30T21:29:59.025537Z"}}, "outputs": [], "source": ["r2_scores.mean()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:59.481283Z", "start_time": "2021-03-30T21:29:59.478948Z"}}, "outputs": [], "source": ["lr2 = LinearRegression()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:29:59.879327Z", "start_time": "2021-03-30T21:29:59.876825Z"}}, "outputs": [], "source": ["cv = ShuffleSplit(random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:00.283345Z", "start_time": "2021-03-30T21:30:00.243093Z"}}, "outputs": [], "source": ["cross_validate(estimator=lr2, X=X, y=y, cv=cv) # The 'cv' parameter can\n", "                                               # also take an integer value"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:00.753672Z", "start_time": "2021-03-30T21:30:00.711057Z"}}, "outputs": [], "source": ["cross_validate(estimator=lr2, X=X, y=y, cv=cv)['test_score']"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:01.175363Z", "start_time": "2021-03-30T21:30:01.135351Z"}}, "outputs": [], "source": ["cross_validate(estimator=lr2, X=X, y=y, cv=cv)['test_score'].mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Regularization - a way to prevent overfitting\n", "    - Types of regularization\n", "        1. Reducing the number of features\n", "        2. Increasing the amount of data\n", "        3. Ridge, Lasso, Elastic Net\n", "        \n", "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data-generating process."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The Strategy Behind Ridge / Lasso / Elastic Net\n", "\n", "Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have **overly large coefficients**.\n", "\n", "The evaluation of many models, linear regression included, proceeds by measuring its **error**, some quantifiable expression of the discrepancy between its predictions and the ground truth. The best-fit line of LR, for example, minimizes the sum of squared residuals.\n", "\n", "Our new idea, then, will be ***to add a term representing the size of our coefficients to our loss function***.\n", "\n", "The goal will still be to minimize this new function, but we can make progress toward this minimum *either* by reducing the size of our residuals *or* by reudcing the size of our coefficients.\n", "\n", "Since coefficients can be either negative or positive, we have the familiar difficulty that we can't simply add them up to get a sense of how large they are in general. Once again there are two natural choices: We could focus either on the squares or the absolute values of the coefficients. The former strategy is the basis for **Ridge** (also called Tikhonov) regularization; the latter strategy results in **LASSO** (Least Absolute Shrinkage and Selection Operator) regularization.\n", "\n", "These tools, as we shall see, are easily implemented with `sklearn`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Changing Our Loss Function\n", "\n", "Overfitting is generally a result of high model variance. High model variance can be caused by:\n", "- having irrelevant or too many predictors\n", "- multicollinearity\n", "- large coefficients\n", "\n", "The first problem is about picking up on noise rather than signal.\n", "The second problem is about having a least-squares estimate that is highly sensitive to random error.\n", "The third is about having highly sensitive predictors.\n", "\n", "Regularization is about introducing a factor into our model designed to enforce the stricture that the coefficients stay small, by penalizing the ones that get too large.\n", "\n", "That is, we'll alter our loss function so that the goal now is not merely to minimize the difference between actual values and our model's predicted values. Rather, we'll add in a term to our loss function that represents the sizes of the coefficients.\n", "\n", "There are two popular ways of doing this:\n", "\n", "Lasso (\"L1\"): Minimize $\\large\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j\\times x_{ij})^2 + \\lambda\\Sigma^{n_{feat.}}_{j=0}|\\beta_j|]$\n", "<br/> <br/>\n", "\n", "Ridge (\"L2\"): Minimize $\\large\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j\\times x_{ij})^2 + \\lambda\\Sigma^{n_{feat.}}_{j=0}\\beta^2_j]$\n", "\n", "**$\\rightarrow$ Don't let these formulas be intimidating.** The first term in each of these (the sum of squares) is the same, and is just the familiar loss function that we've always used. What distinguishes the Lasso Regression from the Ridge Regression is only the extra term on the right. The Lasso uses the absolute values of the coefficients, while the Ridge uses the squares of the coefficients.\n", "\n", "**When should I use each of these?**\n", "\n", "- For a given value of $\\lambda$, the ridge makes for a gentler reining in of runaway coefficients. When in doubt, try ridge first.\n", "- The lasso will more quickly reduce the contribution of individual predictors down to insignificance. It is therefore most useful for trimming through the fat of datasets with many predictors or if a model with very few predictors is especially desirable.\n", "\n", "For a nice discussion of these methods in Python, see [this post](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A quick comparison"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Below, we create some fake data. \n", "\n", "The first columns as our true predictor, and the second column is random noise. The third column is our target."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:27.781261Z", "start_time": "2021-03-30T21:30:27.776877Z"}}, "outputs": [], "source": ["data = np.array([\n", "                [1, 1, 5],\n", "                [2, 2, 4],\n", "                [3, 1, 3],\n", "                [4, 3, 2],\n", "                [5, 1, 1],\n", "               ])\n", "X, y = data[:,:2], data[:,2]\n", "X = X.reshape((len(X), 2))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:30.962421Z", "start_time": "2021-03-30T21:30:30.957200Z"}}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "model = LinearRegression()\n", "model.fit(X,y)\n", "print('No Regularization coeff:', model.coef_)\n", "print('No Regularization intercept:', model.intercept_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:32.660403Z", "start_time": "2021-03-30T21:30:32.655421Z"}}, "outputs": [], "source": ["from sklearn.linear_model import Lasso\n", "lasso = Lasso()\n", "lasso.fit(X,y)\n", "print('Lasso coeff:', lasso.coef_)\n", "print('Lasso intercept:', lasso.intercept_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:30:33.428492Z", "start_time": "2021-03-30T21:30:33.422191Z"}}, "outputs": [], "source": ["from sklearn.linear_model import Ridge\n", "ridge = Ridge()\n", "ridge.fit(X,y)\n", "print('ridge coeff:', ridge.coef_)\n", "print('ridge intercept:', ridge.intercept_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ridge and Lasso Regression\n", "\n", "### Producing an Overfit Model\n", "\n", "We can often produce an overfit model by including **interaction terms**. We'll start over with the penguins dataset. This time we'll include the categorical features."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Train-Test Split"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:28.400640Z", "start_time": "2021-03-30T21:31:28.396185Z"}}, "outputs": [], "source": ["birds = birds.dropna()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:28.872117Z", "start_time": "2021-03-30T21:31:28.866448Z"}}, "outputs": [], "source": ["X = birds.drop('body_mass_g', axis=1)\n", "y = birds['body_mass_g']\n", "X_train, X_test, y_train, y_test = train_test_split(X,y,\n", "                                                   random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:29.421267Z", "start_time": "2021-03-30T21:31:29.401878Z"}}, "outputs": [], "source": ["ohe = OneHotEncoder(drop='first')\n", "dummies = ohe.fit_transform(X_train[['species', 'island', 'sex']])\n", "dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(),\n", "                         index=X_train.index)\n", "X_train_df = pd.concat([X_train[['bill_length_mm', 'bill_depth_mm',\n", "                                'flipper_length_mm']], dummies_df], axis=1)\n", "X_train_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### First simple model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:52.443223Z", "start_time": "2021-03-30T21:31:52.435990Z"}}, "outputs": [], "source": ["test_dummies = ohe.transform(X_test[['species', 'island', 'sex']])\n", "test_df = pd.DataFrame(test_dummies.todense(), columns=ohe.get_feature_names(),\n", "                       index=X_test.index)\n", "X_test_df = pd.concat([X_test[['bill_length_mm', 'bill_depth_mm',\n", "                              'flipper_length_mm']], test_df], axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:54.284487Z", "start_time": "2021-03-30T21:31:54.278395Z"}}, "outputs": [], "source": ["lr1 = LinearRegression()\n", "lr1.fit(X_train_df, y_train)\n", "\n", "pens_preds = lr1.predict(X_test_df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:56.931756Z", "start_time": "2021-03-30T21:31:56.926825Z"}}, "outputs": [], "source": ["lr1.score(X_train_df, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:31:58.279646Z", "start_time": "2021-03-30T21:31:58.273806Z"}}, "outputs": [], "source": ["lr1.score(X_test_df, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:32:04.599615Z", "start_time": "2021-03-30T21:32:04.595292Z"}}, "outputs": [], "source": ["np.sqrt(mean_squared_error(pens_preds, y_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Add Polynomial Features"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:32:19.125815Z", "start_time": "2021-03-30T21:32:19.118989Z"}}, "outputs": [], "source": ["pf = PolynomialFeatures(degree=3)\n", "X_poly_train = pf.fit_transform(X_train_df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:32:46.786935Z", "start_time": "2021-03-30T21:32:46.779075Z"}}, "outputs": [], "source": ["poly_lr = LinearRegression()\n", "poly_lr.fit(X_poly_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:33:00.073239Z", "start_time": "2021-03-30T21:33:00.066970Z"}}, "outputs": [], "source": ["X_poly_test = pf.transform(X_test_df)\n", "poly_preds = poly_lr.predict(X_poly_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:33:01.004146Z", "start_time": "2021-03-30T21:33:00.998623Z"}}, "outputs": [], "source": ["poly_lr.score(X_poly_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That's an improvement in $R^2$ on the training data. Let's check the score on the test data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:33:08.426163Z", "start_time": "2021-03-30T21:33:08.421618Z"}}, "outputs": [], "source": ["poly_lr.score(X_poly_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ugh! What happened?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:33:16.021809Z", "start_time": "2021-03-30T21:33:16.018116Z"}}, "outputs": [], "source": ["np.sqrt(mean_squared_error(poly_preds, y_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In a word, we've overfit our model. Let's see if ridge regularization can help."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ridge (L2) Regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:34:06.050921Z", "start_time": "2021-03-30T21:34:06.040605Z"}}, "outputs": [], "source": ["ss = StandardScaler()\n", "pf = PolynomialFeatures(degree=3)\n", "\n", "# You should always be sure to _standardize_ your data before\n", "# applying regularization!\n", "\n", "X_train_processed = pf.fit_transform(ss.fit_transform(X_train_df))\n", "X_test_processed = pf.transform(ss.transform(X_test_df))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["'Lambda' is the standard variable for the strength of the\n", "regularization (as in the above formulas), but since lambda\n", "is a key word in Python, these sklearn regularization tools\n", "use 'alpha' instead."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:34:58.865142Z", "start_time": "2021-03-30T21:34:58.859824Z"}}, "outputs": [], "source": ["rr = Ridge(alpha=10, random_state=42)\n", "\n", "rr.fit(X_train_processed, y_train)\n", "ridge_preds = rr.predict(X_test_processed)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:34:59.235304Z", "start_time": "2021-03-30T21:34:59.230361Z"}}, "outputs": [], "source": ["rr.score(X_train_processed, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:35:00.133918Z", "start_time": "2021-03-30T21:35:00.129315Z"}}, "outputs": [], "source": ["rr.score(X_test_processed, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:35:16.800620Z", "start_time": "2021-03-30T21:35:16.796136Z"}}, "outputs": [], "source": ["np.sqrt(mean_squared_error(ridge_preds, y_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Much better! But how do we know which value of `alpha` to pick?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Crossvalidation to Optimize the Regularization Hyperparameter\n", "\n", "The regularization strength could sensibly be any nonnegative number, so there's no way to check \"all possible\" values. It's often useful to try several values that are different orders of magnitude."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:36:17.483484Z", "start_time": "2021-03-30T21:36:17.285801Z"}}, "outputs": [], "source": ["alphas = np.linspace(.0001,1000,num=100)\n", "train_scores = []\n", "test_scores = []\n", "\n", "for alpha in alphas:\n", "    rr = Ridge(alpha=alpha, random_state=42)\n", "    rr.fit(X_train_processed, y_train)\n", "    train_score = rr.score(X_train_processed, y_train)\n", "    test_score = rr.score(X_test_processed, y_test)\n", "    \n", "    train_scores.append(train_score)\n", "    test_scores.append(test_score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:36:36.570289Z", "start_time": "2021-03-30T21:36:36.052816Z"}}, "outputs": [], "source": ["plt.style.use('fivethirtyeight')\n", "fig, ax = plt.subplots(figsize=(15,6))\n", "plt.xscale('log')\n", "plt.title('Ridge $R^2$ as a function of regularization strength')\n", "ax.set_xlabel('Regularization strength $\\lambda$')\n", "ax.set_ylabel('$R^2$')\n", "ax.plot(alphas, train_scores, label='train')\n", "ax.plot(alphas, test_scores, label='test')\n", "plt.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It looks like the best value is somewhere around 100. If we wanted more precision, we could repeat the same sort of exercise with a set of alphas nearer to 100.\n", "\n", "#### Observation\n", "Notice how the values increase but then decrease? Regularization helps with overfitting, but if the strength of the regularization becomes too great, then large coefficients will be punished more than they really should. What happens then is that the original error between truth and model predictions becomes neglected as a quantity to be minimized, and the bias of the model begins to outweigh its variance."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Lasso (L1) Regression\n", "\n", "**Exercise**: Produce a similar plot using `Lasso` instead of `Ridge`!\n", "    \n", "    Hint: You may need to increase the value of the 'max_iter' parameter over the default.\n", "    Level Up: Record the coefficients of each model to see how many go to 0 for each value of alpha."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:37:29.470445Z", "start_time": "2021-03-30T21:37:27.159275Z"}}, "outputs": [], "source": ["alphas = np.linspace(.0001,1000,num=100)\n", "train_scores = []\n", "test_scores = []\n", "coefs = []\n", "\n", "\n", "for alpha in alphas:                                          \n", "    lr = Lasso(alpha=alpha, random_state=42, max_iter=100000, tol=.03)\n", "                                                              # http://proceedings.mlr.press/v37/fercoq15.pdf\n", "    lr.fit(X_train_processed, y_train)\n", "    coefs.append(lr.coef_)\n", "    train_score = lr.score(X_train_processed, y_train)\n", "    test_score = cross_val_score(lr, X_test_processed, y_test, cv=3).mean()\n", "    \n", "    train_scores.append(train_score)\n", "    test_scores.append(test_score)\n", "    \n", "fig, ax = plt.subplots(figsize=(15,6))\n", "plt.xscale('log')\n", "plt.title('Lasso $R^2$ as a function of regularization strength')\n", "ax.set_xlabel('Regularization strength $\\lambda$')\n", "ax.set_ylabel('$R^2$')\n", "ax.plot(alphas, train_scores, label='train')\n", "ax.plot(alphas, test_scores, label='test')\n", "plt.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Elastic Net\n", "\n", "There is a combination of L1 and L2 regularization called the Elastic Net that can also be used. The idea is to use a scaled linear combination of the lasso and the ridge, where the weights add up to 100%. We might want 50% of each, but we also might want, say, 10% Lasso and 90% Ridge.\n", "\n", "The loss function for an Elastic Net Regression looks like this:\n", "\n", "Elastic Net: Minimize\n", "\n", "$\\rho\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j\\times x_{ij})^2 + \\lambda\\Sigma^{n_{feat.}}_{j=0}|\\beta_j|] + (1 - \\rho)\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j\\times x_{ij})^2 + \\lambda\\Sigma^{n_{feat.}}_{j=0}\\beta^2_j]$\n", "\n", "Sometimes you will see this loss function represented with different scaling terms, but the basic idea is to have a combination of L1 and L2 regularization terms.\n", "\n", "#### Coding the Elastic Net\n", "\n", "Naturally, the Elastic Net has the same interface through sklearn as the other regularization tools! The only difference is that we now have to specify how much of each regularization term we want. The name of the parameter for this (represented by $\\rho$ above) in sklearn is `l1_ratio`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:38:29.886686Z", "start_time": "2021-03-30T21:38:29.879464Z"}}, "outputs": [], "source": ["enet = ElasticNet(alpha=10, l1_ratio=0.1, random_state=42)\n", "\n", "enet.fit(X_train_processed, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:38:31.552111Z", "start_time": "2021-03-30T21:38:31.546731Z"}}, "outputs": [], "source": ["enet.score(X_train_processed, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:38:33.608222Z", "start_time": "2021-03-30T21:38:33.603497Z"}}, "outputs": [], "source": ["enet.score(X_test_processed, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting the `l1_ratio` to 1 is equivalent to the lasso:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:38:49.928069Z", "start_time": "2021-03-30T21:38:49.925548Z"}}, "outputs": [], "source": ["ratios = np.linspace(0.01, 1, 100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:38:58.150701Z", "start_time": "2021-03-30T21:38:57.947316Z"}}, "outputs": [], "source": ["preds = []\n", "for ratio in ratios:\n", "    enet = ElasticNet(alpha=10, l1_ratio=ratio, random_state=42)\n", "    enet.fit(X_train_processed, y_train)\n", "    preds.append(enet.predict(X_test_processed[0].reshape(1, -1)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:38:59.085549Z", "start_time": "2021-03-30T21:38:58.925361Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "\n", "lasso = Lasso(alpha=10, random_state=42)\n", "lasso.fit(X_train_processed, y_train)\n", "lasso_pred = lasso.predict(X_test_processed[0].reshape(1, -1))\n", "\n", "ax.plot(ratios, preds, label='elastic net')\n", "ax.scatter(1, lasso_pred, c='k', s=70, label='lasso')\n", "plt.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Note on `ElasticNet()`\n", "\n", "Is an Elastic Net with `l1_ratio` set to 0 equivalent to the ridge? In theory yes. But in practice no. It looks like the `ElasticNet()` predictions on the first test data point as `l1_ratio` shrinks are tending toward some value just above 3700. Let's check to see what prediction `Ridge()` gives us:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:39:35.628109Z", "start_time": "2021-03-30T21:39:35.620815Z"}}, "outputs": [], "source": ["ridge = Ridge(alpha=10, random_state=42)\n", "ridge.fit(X_train_processed, y_train)\n", "ridge.predict(X_test_processed[0].reshape(1, -1))[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you check the docstring for the `ElasticNet()` class you will see:\n", "- that the function being minimized is slightly different from what we saw above; and\n", "- that the results are unreliable when `l1_ratio` $\\leq 0.01$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise**: Visualize the difference in this case between `ElasticNet(l1_ratio=0.01)` and `Ridge()` by making a scatterplot of each model's predicted values for the first ten points in `X_test_processed`. Use `alpha=10` for each model. This plot should have the integers 1-10 on the x-axis, and the prediction values on the y-axis.\n", "\n", "        Level Up: Make a second scatterplot that compares the predictions on the same data\n", "        points between ElasticNet(l1_ratio=1) and Lasso()."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Fitting Regularized Models with Cross-Validation\n", "\n", "Our friend `sklearn` also includes tools that fit regularized regressions *with cross-validation*: `LassoCV`, `RidgeCV`, and `ElasticNetCV`.\n", "\n", "**Exercise**: Use `RidgeCV` to fit a seven-fold cross-validated ridge regression model to our `X_train_processed` data and then calculate $R^2$ and the RMSE (root-mean-squared error) on our test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:40:17.714975Z", "start_time": "2021-03-30T21:40:17.654676Z"}}, "outputs": [], "source": ["rcv = RidgeCV(cv=7)\n", "rcv.fit(X_train_processed, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:40:18.638481Z", "start_time": "2021-03-30T21:40:18.633978Z"}}, "outputs": [], "source": ["rcv.score(X_test_processed, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:40:19.447218Z", "start_time": "2021-03-30T21:40:19.442370Z"}}, "outputs": [], "source": ["np.sqrt(mean_squared_error(y_test, rcv.predict(X_test_processed)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:40:25.737284Z", "start_time": "2021-03-30T21:40:25.733458Z"}}, "outputs": [], "source": ["rcv.alpha_"]}, {"cell_type": "markdown", "metadata": {"ExecuteTime": {"end_time": "2021-03-30T18:38:47.852281Z", "start_time": "2021-03-30T18:38:47.849168Z"}}, "source": ["# Using IC scores"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IC scores are a probabilistic way of model and feature selection.\n", "\n", "IC scores tend to favor models that are slightly underfit and heavily penalize models as their complexity increases (as the complexity increases the likelihood of them being overfit also increases).\n", "\n", "When your sample size is too small for cross validation IC scores can be used to "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:41:03.217453Z", "start_time": "2021-03-30T21:41:03.214393Z"}}, "outputs": [], "source": ["def aic(n_samples, k, error, y):\n", "    return n_samples * error / np.var(y) + k\n", "\n", "def bic(n, k, error):\n", "    return n * np.log(error/n) + k * np.log(n)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:41:04.427231Z", "start_time": "2021-03-30T21:41:03.548047Z"}}, "outputs": [], "source": ["alphas = np.linspace(.0001,100,num=100)\n", "aic_scores = []\n", "bic_scores = []\n", "\n", "full_ohe = OneHotEncoder(drop='first')\n", "dummies = full_ohe.fit_transform(X[['species', 'island', 'sex']])\n", "dummies_df = pd.DataFrame(dummies.todense(), columns=full_ohe.get_feature_names(),\n", "                         index=X.index)\n", "\n", "X_ohe = pd.concat([X[['bill_length_mm', 'bill_depth_mm',\n", "                                'flipper_length_mm']], dummies_df], axis=1)\n", "\n", "\n", "ss = StandardScaler()\n", "\n", "X_trans = ss.fit_transform(X_ohe)\n", "\n", "for alpha in alphas:\n", "    rr = Lasso(alpha=alpha, random_state=42)\n", "    rr.fit(X_trans, y)\n", "    preds = rr.predict(X_trans)\n", "    sse = ((y-preds)**2).sum()\n", "    k = sum([1 for x in rr.coef_ if round(abs(x), 5) > 0])\n", "    aic_scores.append(aic(X_trans.shape[0], k, sse, y))\n", "    bic_scores.append(bic(len(X_trans), k, sse))\n", "    \n", "\n", "fig, axes = plt.subplots(1,2, figsize=(15,5))\n", "axes[0].semilogx(alphas, aic_scores, '--', color='red',\n", "                 linewidth=3, label='AIC')\n", "axes[1].semilogx(alphas, bic_scores, '--', color='blue',\n", "                 linewidth=3, label='BIC');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Using Sklearn"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T21:41:20.379125Z", "start_time": "2021-03-30T21:41:19.948584Z"}}, "outputs": [], "source": ["from sklearn.linear_model import LassoLarsIC\n", "def plot_ic_criterion(model, name, color):\n", "    criterion_ = model.criterion_\n", "    plt.semilogx(model.alphas_ + EPSILON, criterion_, '--', color=color,\n", "                 linewidth=3, label='%s criterion' % name)\n", "    plt.axvline(model.alpha_ + EPSILON, color=color, linewidth=3,\n", "                label='alpha: %s estimate' % name)\n", "    plt.xlabel(r'$\\alpha$')\n", "    plt.ylabel('criterion')\n", "\n", "EPSILON = 1e-4\n", "model_bic = LassoLarsIC(criterion='bic')\n", "model_bic.fit(X_trans, y)\n", "alpha_bic_ = model_bic.alpha_\n", "\n", "model_aic = LassoLarsIC(criterion='aic')\n", "model_aic.fit(X_trans, y)\n", "alpha_aic_ = model_aic.alpha_\n", "\n", "plt.figure(figsize=(15,6))\n", "plot_ic_criterion(model_aic, 'AIC', 'b')\n", "plot_ic_criterion(model_bic, 'BIC', 'r')\n", "plt.legend()\n", "plt.title('Information-criterion for model selection')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-03-30T19:08:43.558172Z", "start_time": "2021-03-30T19:08:43.554525Z"}}, "outputs": [], "source": ["model_aic.alpha_"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}