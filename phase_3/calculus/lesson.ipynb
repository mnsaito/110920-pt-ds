{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus\n",
    "\n",
    "In the last study group, we saw how Linear Regression can be solved algebraically. But sometimes this computation is too expensive. And for other sorts of loss functions, it may not be available at all. So we turn to numerical methods, and, in particular, to the method of gradient descent. In order to gain a good grasp of this method, we'll have to review a couple items from calculus, namely:\n",
    "\n",
    "- Derivatives\n",
    "- The Chain Rule\n",
    "- Partial Differentiation.\n",
    "\n",
    "\n",
    "### Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T19:28:11.971073Z",
     "start_time": "2021-03-23T19:28:11.967217Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vT_gWO1_WmTjXjJPbWREXYDzPn3EQScIxWcw3mJHeKhZWwfGtWlJdD-xqwAbltCDlVnNZicDhq_2h7A/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"480\" height=\"299\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chain Rule\n",
    "\n",
    "$\\large\\frac{d}{dx}[f(g(x))] = f'(g(x))g'(x)$\n",
    "\n",
    "That is: The derivative of a *composition* of functions is: the derivative of the first applied to the second, multiplied by the derivative of the second.\n",
    "\n",
    "So if we know e.g. that $\\frac{d}{dx}[e^x] = e^x$ and $\\frac{d}{dx}[x^2] = 2x$, then we can use the Chain Rule to calculate $\\frac{d}{dx}[e^{x^2}]$. We set $f(x) = e^x$ and $g(x) = x^2$, so the derivative must be:\n",
    "\n",
    "$\\large\\frac{d}{dx}[e^{x^2}] = (e^{x^2})(2x) = 2xe^{x^2}$.\n",
    "\n",
    "### Partial Differentiation\n",
    "\n",
    "Partial differentiation is required for functions of multiple variables. If e.g. I have some function $h = h(a, b)$, then I can consider how $h$ changes with respect to $a$ (while keeping $b$ constant)––that's $\\frac{\\partial h}{\\partial a}$, and I can consider how $h$ changes with respect to $b$ (while keeping $a$ constant)––that's $\\frac{\\partial h}{\\partial b}$. And so the rule is simple enough: If I'm differentiating my function with respect to some variable, I'll **treat all other variables as constants**.\n",
    "\n",
    "So e.g. if \n",
    "\n",
    "Suppose our loss function looks like this:\n",
    "\n",
    "$\\large\\xi(x, y, z) = x^2y^5z^3 - ze^{cos(xy)} + (yz)^3$;\n",
    "\n",
    "for some parameters $x$, $y$, and $z$.\n",
    "\n",
    "What are the partial derivatives of this function?\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial x} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $2xy^5z^3 + yze^{cos(xy)}sin(xy)$\n",
    "    </details>\n",
    "<br/>\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial y} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $5x^2y^4z^3 + xze^{cos(xy)}sin(xy) + 3y^2z^3$\n",
    "    </details>\n",
    "<br/>\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial z} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $3x^2y^5z^2 - e^{cos(xy)} + 3y^3z^2$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Suppose that we have these data points:\n",
    "\n",
    "(1, 2), (3, 9), (5, 10),\n",
    "\n",
    "and that we're interested in constructing a best-fit line through them.\n",
    "\n",
    "Now this optimization problem has a well-known solution and we could simply plug in our values here into that formula.\n",
    "\n",
    "The solution to a linear regression problem is a matter of minimizing the sum of squared distances between actual y-values and the line's predictions.\n",
    "\n",
    "More generally, model optimization involves setting the derivative of the function you want to minimize (for a linear regression, this would be the sum of the squared distances between $y$ and $\\hat{y}$) to zero, and then solving for the parameters that define the model (for a linear regression, this would be the slope and y-intercept).\n",
    "\n",
    "But this is not always easily or efficiently done. Sometimes the loss function is quite complicated and it is not a straightforward matter to set the derivative equal to zero and to solve the resulting equation(s) for the missing parameters.\n",
    "\n",
    "Sometimes it's easier or more efficient to:\n",
    "\n",
    "#### Gradient Descent in Words\n",
    "- make a guess at where the function attains its minimum value; and then\n",
    "- calculate the derivative at that point; and then\n",
    "- use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, in the single-variable case, a positive derivative indicates an increasing function and a negative derivative indicates a decreasing function.\n",
    "\n",
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. And that means that **the gradient will point in the direction of steepest increase**.\n",
    "\n",
    "Let's look back at our [3-d plotter](https://academo.org/demos/3d-surface-plotter/).\n",
    "\n",
    "Therefore, if we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient*\n",
    "of our loss function to *descend* to the minimum value of that loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let's try this with our three data points above.\n",
    "\n",
    "You may recall that the parameters that produce the best-fit line for the three points above are:\n",
    "\n",
    "$\\beta_0 = 1$; <br/>\n",
    "$\\beta_1 = 2$.\n",
    "\n",
    "But suppose we try to arrive at these values by guessing and checking, i.e. by the method of gradient descent.\n",
    "\n",
    "Let's start with a guess of:\n",
    "\n",
    "$\\beta_0 = 3$; <br/>\n",
    "$\\beta_1 = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:35:07.990276Z",
     "start_time": "2021-03-23T20:35:07.987571Z"
    }
   },
   "outputs": [],
   "source": [
    "beta_0 = 3\n",
    "beta_1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function we're trying to minimize is:\n",
    "\n",
    "$\\large f(\\beta_0, \\beta_1) = \\Sigma(y - \\beta_1x - \\beta_0)^2$.\n",
    "\n",
    "So we have:\n",
    "\n",
    "$\\large\\frac{\\partial f}{\\partial\\beta_0} = -2\\Sigma(y - \\beta_1x - \\beta_0)$; and\n",
    "\n",
    "$\\large\\frac{\\partial f}{\\partial\\beta_1} = -2\\Sigma x(y - \\beta_1x - \\beta_0)$.\n",
    "\n",
    "Note that if we plug in $\\beta_0$ = 1, $\\beta_1$ = 2, we get 0 for these derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfdbeta0(beta_0, beta_1):\n",
    "    return -2 * ((2 - beta_1*1 - beta_0) + (9 - beta_1*3 - beta_0) + (10 - beta_1*5 - beta_0))\n",
    "\n",
    "def dfdbeta1(beta_0, beta_1):\n",
    "    return -2 * ((1 * (2 - beta_1*1 - beta_0)) + (3 * (9 - beta_1*3 - beta_0)) + (5 * (10 - beta_1*5 - beta_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(1, 2)\n",
    "partial_1 = dfdbeta1(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:36:56.429466Z",
     "start_time": "2021-03-23T20:36:56.420439Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plug in our guesses and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:37:18.297974Z",
     "start_time": "2021-03-23T20:37:18.294112Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(3, 3)\n",
    "partial_1 = dfdbeta1(3, 3)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\frac{\\partial f}{\\partial\\beta_0}$ and $\\frac{\\partial f}{\\partial\\beta_1}$ are positive, this tells us that we need to make our guesses **smaller** for each. So we might try $\\beta_0 = \\beta_1 = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:38:11.809413Z",
     "start_time": "2021-03-23T20:38:11.805348Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(1, 1)\n",
    "partial_1 = dfdbeta1(1, 1)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make our guesses **larger**! Note that, even though $\\beta_0 = 1$ is part of the optimal solution, there is no guarantee that the associated partial derivative will be 0 for all the points in parameter space where $\\beta_0 = 1$, since the derivative is a function both of $\\beta_0$ and of $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Size\n",
    "\n",
    "How can we guarantee that we don't step **too far** when making a new estimate of the optimal parameter values? If we step too far we may never reach the minimum. And for that matter, there is reason also not to have our step size be **too small**. If we're far from the minimum, then a small learning rate would make our route to the minimum very costly (in terms of computation time, if nothing else).\n",
    "\n",
    "Here's an elegant solution: Make the size of your step proportional to the value of the derivative at the point where you currently are in parameter space! If we're very far from the minimum, then our values will be large, and so we therefore can safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "I said the size of the step is proportional to the value of the derivative. The constant of proportionality is often called the **\"learning rate\"**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page helps to explain the dangers of learning rates that are too large and too small: https://www.jeremyjordan.me/nn-learning-rate/.\n",
    "\n",
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula\n",
    "\n",
    "The general algorithm looks like this:\n",
    "\n",
    "We'll make a guess, $\\vec{s}$, at where our loss function attains a minimum. If we're not happy with how close the value of the gradient there is to 0, then we'll make a new guess, and the new guess will be constructed as follows:\n",
    "\n",
    "$\\large\\vec{s}_{new} = \\vec{s}_{old} - \\alpha\\nabla f(\\vec{s}_{old})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Let's write a function that will utilize gradient descent for a simple one-parameter loss function.\n",
    "\n",
    "The inputs will be:\n",
    "\n",
    "- a function representing the derivative of our loss function;\n",
    "- an initial guess;\n",
    "- a learning rate;\n",
    "- a level of desired precision\n",
    "- a maximum number of iterations to run through before quitting\n",
    "\n",
    "I've started the code for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_d_grad_desc(deriv, guess, alpha=0.001, prec=0.000001, max_iter=10**5):\n",
    "    \n",
    "    # Update parameter max_iter number times \n",
    "    for _ in range(max_iter):\n",
    "        # Create an `old_x` variable that is copy of `guess`\n",
    "        \n",
    "        # Update guess by subtracting alpha*derivative of old_x\n",
    "        \n",
    "        # Create a `step` variable that is the difference\n",
    "        # between `guess` and `old_x`\n",
    "            # If step is smaller than the desired precision\n",
    "            # break the loop\n",
    "        pass\n",
    "    \n",
    "    # Return the updated parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test It!\n",
    "\n",
    "Once you've got the function coded, try it out on some simple one-dimensional functions!\n",
    "\n",
    "Try it on:\n",
    "\n",
    "- `deriv` = $2x$. What answer should you get here?\n",
    "\n",
    "- `deriv` = $10^x - 100$. What answer should you get here?\n",
    "\n",
    "- **loss function** = $3x^3 - 3x^2 + x - 2$\n",
    "\n",
    "- **loss function** = $(4x + 5)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:09:11.178100Z",
     "start_time": "2021-03-23T20:09:11.173051Z"
    }
   },
   "outputs": [],
   "source": [
    "one_d_grad_desc(deriv=lambda x: 2*x, guess=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:09:13.539149Z",
     "start_time": "2021-03-23T20:09:13.534427Z"
    }
   },
   "outputs": [],
   "source": [
    "one_d_grad_desc(deriv=lambda x: 10**x - 100, guess=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:09:31.330926Z",
     "start_time": "2021-03-23T20:09:31.322680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function = 3x^3 - 3x^2 + x - 2\n",
    "# So derivative = 9x^2 - 6x + 1\n",
    "\n",
    "# If 9x^2 - 6x + 1 = 0, then x = 1/3\n",
    "\n",
    "one_d_grad_desc(deriv=lambda x: 9*x**2 - 6*x + 1, guess=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T20:09:46.320745Z",
     "start_time": "2021-03-23T20:09:46.316828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function = (4x + 5)^2\n",
    "# So derivative = (2*4)(4x + 5) = 32x + 40\n",
    "\n",
    "# If 32x + 40 = 0, then x = -5/4\n",
    "\n",
    "one_d_grad_desc(deriv=lambda x: 32*x + 40, guess=-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
