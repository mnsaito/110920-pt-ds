{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pipelines and Pickles\n",
    "> How to use transformers with sklearn's `cross_val_score` and export a machine learning model from a jupyter notebook.\n",
    "\n",
    "<img src=\"https://www.polyeurope.com/site/images/home_slider/pig_animation.gif\">\n",
    "\n",
    "#### Motivations:\n",
    "\n",
    "> We have learned how to train a machine learning model, and *we have discussed why we should never fit on our testing data*, but this becomes more complicated when we are using multiple fold cross validation ie breaking data into multiple splits. The solution for this is either a lot of manual code, or a pipeline.\n",
    "\n",
    ">What if we wanted to deploy a model? **Currently,** our modeling process is broken up into a bunch of different code cells within a jupyter notebook, and there is no way for us to ask the model to make predictions on new data without opening the notebook, running the code to train the model, importing new data, and manually running code cells to make predictions. Google data scientists are not sitting at their computer running `shift + enter` for the thousands of data points they receive every second. So how to we use our models outside of a jupyter notebook? The solution to that is the `pickle` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u> Students will be able to </u></h3>\n",
    "\n",
    "- Understand why pipelines are useful for model evaluation.\n",
    "- Learn to construct a simple pipeline.\n",
    "- Save a model or a pipeline as a `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:48:10.506581Z",
     "start_time": "2021-04-22T14:48:10.485427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Transformers\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# Modeling Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "\n",
    "# Mock Data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's review the modeling workflow we've seen so far...\n",
    "\n",
    "In the cell below, we load the breast cancer dataset into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:20:41.197112Z",
     "start_time": "2021-04-22T14:20:41.166044Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data['data'], columns = data['feature_names'])\n",
    "df['target'] = data['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*This particular dataset is used mostly for demonstration purposes so there is no cleaning required.* ***Normally,*** *the next step would be for us to clean and format our data so it can then be modeled. Because the data is clean, we will skip this step.*\n",
    "\n",
    "Next, we split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:20:42.683140Z",
     "start_time": "2021-04-22T14:20:42.676994Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Predictive Features\n",
    "features = df.drop('target', axis = 1)\n",
    "# Target Variable\n",
    "target = df.target\n",
    "\n",
    "# Create two sets of data. \n",
    "# One for training a model \n",
    "# One for predicting on unseen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:20:43.686561Z",
     "start_time": "2021-04-22T14:20:43.683088Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Logistic Regression as our first model. Because Sklearn uses L2 regularization with Logistic Regression, we'll want to scale our data.\n",
    "\n",
    "There are many ways to scale data. Today we will use sklearn's `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:21:57.626627Z",
     "start_time": "2021-04-22T14:21:57.616234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a scaler to our training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:22:00.185211Z",
     "start_time": "2021-04-22T14:22:00.182735Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use cross validation to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:22:05.888978Z",
     "start_time": "2021-04-22T14:22:05.858744Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(model, X_train_scaled, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have done this, we would likely make some alterations to the data or fit other models and compare each model's performance. But let's say that we decide this is our final model. The next thing we would do is test the model on our testing data.\n",
    "\n",
    "To do this, we fit the model to all of our training data, and evaluate performance on the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:23:13.005722Z",
     "start_time": "2021-04-22T14:23:12.993221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit final model on all training data\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:23:49.152110Z",
     "start_time": "2021-04-22T14:23:49.145354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score model\n",
    "print('Training Accuracy: ', model.score(X_train_scaled, y_train))\n",
    "print('Testing Accuracy: ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did we do wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:15:45.189096Z",
     "start_time": "2020-10-09T01:15:45.185953Z"
    }
   },
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T06:20:59.554642Z",
     "start_time": "2020-10-09T06:20:59.522017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Isolate features from target\n",
    "features = df.drop('target', axis = 1)\n",
    "target = df.target\n",
    "\n",
    "# Treain test splitdd\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=2020)\n",
    "\n",
    "# Scale training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = features.columns)\n",
    "\n",
    "# Fit model on training data\n",
    "model = LogisticRegression(random_state=2020)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to use the model on testing data, *we have to transform the test data with the transformer that has been fit on the training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T06:20:59.562499Z",
     "start_time": "2020-10-09T06:20:59.556989Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare how the model performs on the unscaled and scaled testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T06:20:59.572115Z",
     "start_time": "2020-10-09T06:20:59.564726Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Unscaled ', model.score(X_test, y_test))\n",
    "print('Scaled ', model.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we decide that this is our final model, we refit our transformer and model to all of our data, and save the scaler and model to disk using the `pickle` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:30:55.389776Z",
     "start_time": "2021-04-22T14:30:55.368755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit to all of our data\n",
    "scaler.fit(features)\n",
    "X = scaler.transform(features)\n",
    "model.fit(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:32:29.044335Z",
     "start_time": "2021-04-22T14:32:29.039765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save scaler\n",
    "file = open('scaler.pkl', 'wb')\n",
    "pickle.dump(scaler, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:32:57.234453Z",
     "start_time": "2021-04-22T14:32:57.230567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "file = open('final_model.pkl', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But, what if we wanted to use multiple fold cross validation.** If the correct way to scale data is to only fit the transformer on the training data, how do we use `cross_val_score`?\n",
    "\n",
    "Under the hood, `cross_val_score` is making splits of the data we feed into the function, training on one split and making predictions on the other. For this to work with transformers, we would need to stick a transformer into the middle of that process. \n",
    "\n",
    "We could choose not to use `cross_val_score` but that puts us at risk of training or testing our model on a bad sample of data. After all, when we only use a single train test split there is a possibility that our split of the data, by random chance, happens to result in an unusually high or unusually low accuracy score. We don't want that. We want to report the most representative score for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:41:24.390793Z",
     "start_time": "2021-04-22T14:41:23.087363Z"
    }
   },
   "outputs": [],
   "source": [
    "_test_scores = []\n",
    "for i in range(100):\n",
    "    _xtrain, _xtest, _ytrain, _ytest = train_test_split(features, \n",
    "                                                        target, \n",
    "                                                        random_state=i)\n",
    "    _scaler = StandardScaler()\n",
    "    _xtrain = _scaler.fit_transform(_xtrain)\n",
    "    _xtest = _scaler.transform(_xtest)\n",
    "    _model = LogisticRegression(random_state=2021)\n",
    "    _model.fit(_xtrain, _ytrain)\n",
    "    _test_score = _model.score(_xtest, _ytest)\n",
    "    _test_scores.append(_test_score)\n",
    "\n",
    "plt.figure(figsize=(20,6))   \n",
    "plt.bar([x for x in range(len(_test_scores))], _test_scores)\n",
    "plt.hlines(np.mean(_test_scores), 0, 100, \n",
    "           color='red', linewidth=3, label='Average')\n",
    "plt.title('Testing Scores', size=30)\n",
    "legend = plt.legend(loc='lower left', fontsize=20)\n",
    "legend.get_frame().set_alpha(None)\n",
    "plt.xlabel('Splits of the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T15:27:30.018508Z",
     "start_time": "2020-10-07T15:27:28.242980Z"
    },
    "code_folding": []
   },
   "source": [
    "<u>In summary we need to be able to do the following:</u>\n",
    "1. Access splits made by sklearn's `cross_val_score`\n",
    "2. Fit the desired Transformer on the training split\n",
    "3. Transform the training data\n",
    "4. Fit the desired model on the training data\n",
    "5. Transform the testing data with the fit transformer <u>(Never fit on the testing data)</u>\n",
    "6. Use our fit model to make predictions on the transformed testing data.\n",
    "\n",
    "<img src=\"images/cross_validation.gif\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T15:49:31.824064Z",
     "start_time": "2020-10-07T15:49:31.590736Z"
    }
   },
   "source": [
    "# Introducing sklearn's pipeline\n",
    "\n",
    "![](https://raw.githubusercontent.com/learn-co-students/pickles-and-pipelines-seattle-ds-012720/4817cebf95395b6f10ad882bf9daac528a0650dc/visuals/transformer.gif)\n",
    "\n",
    "\n",
    "With a pipeline, this code...\n",
    "\n",
    "```python\n",
    "transformer.fit(X_train)\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "model.fit(X_train_transformed)\n",
    "model.score(X_test_transformed)\n",
    "```\n",
    "\n",
    "...instead, looks like this:\n",
    "\n",
    "```python\n",
    "pipeline.fit(X_train)\n",
    "pipeline.score(X_test)\n",
    "```\n",
    "\n",
    "#### Benefits of the Pipeline\n",
    "\n",
    "- **Convenience and encapsulation**\n",
    "    - You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "- **Joint parameter selection**\n",
    "     - You can grid search over parameters of all objects in the pipeline at once.\n",
    "- **Safety**\n",
    "    - Pipelines help avoid leaking statistics from your test data into the trained model during cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "\n",
    "Let's create a Pipeline objects to do the following:\n",
    "- Scale data using `StandardScaler`\n",
    "- Fit a Logistic Regression model\n",
    "- Score on the testing data\n",
    "\n",
    "Sklearn makes this process very simple with their `make_pipeline` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:49:29.201647Z",
     "start_time": "2021-04-22T14:49:29.195574Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "first_pipeline = make_pipeline(StandardScaler(), \n",
    "                         LogisticRegression(random_state=2021))\n",
    "first_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed the pipeline into sklearn's `cross_val_score`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:49:35.479057Z",
     "start_time": "2021-04-22T14:49:35.400038Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(first_pipeline, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! The nice thing about pipelines is that it allows us to create and easily access different modeling strategies! We can even feed pipelines into other pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:49:41.513495Z",
     "start_time": "2021-04-22T14:49:41.510346Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing = make_pipeline(PolynomialFeatures(interaction_only=True), \n",
    "                              StandardScaler())\n",
    "\n",
    "second_pipeline = make_pipeline(preprocessing, \n",
    "                              LogisticRegression(random_state=2021))                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:49:47.157151Z",
     "start_time": "2021-04-22T14:49:47.013089Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(second_pipeline, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:49:52.213281Z",
     "start_time": "2021-04-22T14:49:52.195555Z"
    }
   },
   "outputs": [],
   "source": [
    "first_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:49:59.789115Z",
     "start_time": "2021-04-22T14:49:59.781607Z"
    }
   },
   "outputs": [],
   "source": [
    "first_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling!\n",
    "\n",
    "From the [documentation](https://docs.python.org/3/library/pickle.html):\n",
    "\n",
    ">The pickle module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. Pickling (and unpickling) is alternatively known as “serialization”, “marshalling,” 1 or “flattening”; however, to avoid confusion, the terms used here are “pickling” and “unpickling”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to save a machine learning model\n",
    "\n",
    "1. Use python's built in `open` function to open a new file and save the opened file to a variable\n",
    "    - This function receives:\n",
    "        1. The name of the file you would like to open\n",
    "        2. The mode in which you would like to open the file\n",
    "            - For pickle the mode is `'wb'` which stands for \"write binary\".\n",
    "            \n",
    "2. Use `pickle.dump()` \n",
    "    - Feed into this function the object you would like to save (in this case, our model) and the variable in which we have saved the opened file. \n",
    "    \n",
    "3. Close the opened file by calling `.close()` on the file object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:50:58.358592Z",
     "start_time": "2021-04-22T14:50:58.339630Z"
    }
   },
   "outputs": [],
   "source": [
    "first_pipeline.fit(features, target)\n",
    "\n",
    "file = open('models/final_pipeline.pkl', 'wb')\n",
    "pickle.dump(first_pipeline, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to load a saved machine learning model\n",
    "\n",
    "1. Use python's build in `open()` function to open the file and save the opened file to a variable.\n",
    "    - Two things must be fed into this function:\n",
    "        1. The path to the file\n",
    "        2. The mode in which you would like to open the file. \n",
    "            - In this case, we will want to use `'rb'` which stands for \"read binary\".\n",
    "            \n",
    "2. Feed the opened file to `pickle.load()` to load in the model and save the loaded object to a variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:51:08.175465Z",
     "start_time": "2021-04-22T14:51:08.172207Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('models/final_pipeline.pkl', 'rb')\n",
    "saved_model = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:51:08.618843Z",
     "start_time": "2021-04-22T14:51:08.614028Z"
    }
   },
   "outputs": [],
   "source": [
    "saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Topic - Applying a transformer to a specific column in a pipelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with sklearn's `make_column_transformer`.\n",
    "\n",
    "Let's import some data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:47:47.189445Z",
     "start_time": "2021-04-22T18:47:47.075469Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:28:23.627263Z",
     "start_time": "2021-04-22T18:28:23.624805Z"
    }
   },
   "source": [
    "Let's say we isolate the first two columns plus our targer. `age`  `workclass` and `income`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:47:48.414004Z",
     "start_time": "2021-04-22T18:47:48.404450Z"
    }
   },
   "outputs": [],
   "source": [
    "df_small = df[['age', 'workclass', 'income']]\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to *only* apply a OneHotEncoder to workclass we can use `make_column_tranformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:47:54.669560Z",
     "start_time": "2021-04-22T18:47:54.666965Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_column_transformer` excepts tuples containing  a single transformer objects and a list of columns we would like to apply the transformation to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:47:55.465588Z",
     "start_time": "2021-04-22T18:47:55.462708Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_transformer = make_column_transformer((OneHotEncoder(sparse=False), \n",
    "                                          ['workclass']), \n",
    "                                         remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:47:56.190514Z",
     "start_time": "2021-04-22T18:47:56.154035Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_transformer.fit_transform(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:48:05.369101Z",
     "start_time": "2021-04-22T18:48:05.354612Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "df_small.income = target_encoder.fit_transform(df.income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:48:10.634202Z",
     "start_time": "2021-04-22T18:48:09.437692Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(cat_transformer, LogisticRegression())\n",
    "\n",
    "cross_val_score(pipeline, df_small.drop(columns='income'), df.income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:48:20.747982Z",
     "start_time": "2021-04-22T18:48:20.744118Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T18:48:47.688424Z",
     "start_time": "2021-04-22T18:48:47.686250Z"
    }
   },
   "source": [
    "## Applying custom transformers to columns with the same datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:05:37.905374Z",
     "start_time": "2021-04-22T19:05:37.900637Z"
    }
   },
   "outputs": [],
   "source": [
    "df_small = df[['age', 'workclass', 'gender', 'hours-per-week', 'income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:05:38.219358Z",
     "start_time": "2021-04-22T19:05:38.216546Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "number_selector = make_column_selector(dtype_include='number', dtype_exclude='object')\n",
    "string_selector = make_column_selector(dtype_include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:07:47.986069Z",
     "start_time": "2021-04-22T19:07:47.942672Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer((OneHotEncoder(handle_unknown='ignore'), string_selector),\n",
    "                                        (StandardScaler(), number_selector),\n",
    "                                       remainder='passthrough')\n",
    "\n",
    "\n",
    "preprocessing.fit_transform(df_small.drop(columns='income'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__preprocessing = make_column_transformer((OneHotEncoder(handle_unknown='ignore'), string_selector),\n",
    "                                        (StandardScaler(), number_selector),\n",
    "                                       remainder='passthrough')\n",
    "\n",
    "\n",
    "preprocessing.fit_transform(df_small.drop(columns='income'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:07:52.956269Z",
     "start_time": "2021-04-22T19:07:52.952214Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing.transformers_[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add our preprocessing pipeline to a pipeline with an estimator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:07:59.952706Z",
     "start_time": "2021-04-22T19:07:58.552761Z"
    }
   },
   "outputs": [],
   "source": [
    "full_pipeline = make_pipeline(preprocessing, LogisticRegression())\n",
    "\n",
    "cross_val_score(full_pipeline, df_small.drop(columns='income'), df_small.income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying custom transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:08:44.390340Z",
     "start_time": "2021-04-22T19:08:44.387953Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:08:44.662119Z",
     "start_time": "2021-04-22T19:08:44.658508Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def is_middle_age(x):\n",
    "    if x < 45:\n",
    "        return 0 \n",
    "    elif x > 64:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1\n",
    "    \n",
    "is_middle_age = np.vectorize(is_middle_age)\n",
    "\n",
    "MiddleAgeTransformer = FunctionTransformer(is_middle_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:12:30.591531Z",
     "start_time": "2021-04-22T19:12:30.588229Z"
    }
   },
   "outputs": [],
   "source": [
    "def works_full_time(x):\n",
    "    if x >= 40:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "works_full_time = np.vectorize(works_full_time)\n",
    "\n",
    "FullTimeTransformer = FunctionTransformer(works_full_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:08:48.839783Z",
     "start_time": "2021-04-22T19:08:48.828378Z"
    }
   },
   "outputs": [],
   "source": [
    "MiddleAgeTransformer.transform(df.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:12:45.642336Z",
     "start_time": "2021-04-22T19:12:45.631193Z"
    }
   },
   "outputs": [],
   "source": [
    "FullTimeTransformer.transform(df['hours-per-week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:14:12.888208Z",
     "start_time": "2021-04-22T19:14:12.824789Z"
    }
   },
   "outputs": [],
   "source": [
    "column_transformer = make_column_transformer((MiddleAgeTransformer, ['age']),\n",
    "                                             (FullTimeTransformer, ['hours-per-week']),\n",
    "                                             (OneHotEncoder(handle_unknown='ignore'), string_selector))\n",
    "\n",
    "column_transformer.fit_transform(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:15:17.049238Z",
     "start_time": "2021-04-22T19:15:17.046348Z"
    }
   },
   "outputs": [],
   "source": [
    "full_pipeline = make_pipeline(column_transformer, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:15:48.974601Z",
     "start_time": "2021-04-22T19:15:47.720087Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(full_pipeline, df_small.drop(columns='income'), df.income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Time - Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline for the entire adult dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
