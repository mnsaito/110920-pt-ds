{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import pandas as pd\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Learning goals\n", "\n", "1. Compare predicting a continuous outcome to predicting a class\n", "2. Compare linear to logistic regression as classification models\n", "3. Describe how logistic regression works under the hood\n", "4. Learn how to interpret a trained logistic model's coefficients\n", "6. Explore the C (inverse regularization) paramater and hyperparameter tune\n", "7. Describe the assumptions of logistic regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Why logistic regression as the 1st of our classifiers?\n", "\n", "- It is widely used.\n", "\n", "- Logistic regression takes a concept we are familiar with, a linear equation, and translates it into a form fit for predicting a class.  \n", "\n", "- It generally can't compete with the best supervised learning algorithms, but it is **simple, fast, and interpretable**.  \n", "\n", "- As we will see in mod 4, it will also serve as a segue into our lessons on **neural nets**.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Compare predicting a continuous outcome to predicting a class\n", "\n", "Thus far, we have worked to predict continuous target variables using linear regression. \n", "\n", "  - Continous target variables:\n", "        - Sales price of a home\n", "        - MPG of a car\n", "        - A country's life expectancy rate\n", "        \n", "We will now transition into another category of prediction: classification. Instead of continous target variables, we will be predicting whether records from are data are labeled as a particular class.  Whereas the output for the linear regression model can be any number, the output of our classification algorithms can only be a value designated by a set of discrete outcomes.\n", "\n", "  - Categorical target variables:\n", "        - Whether an employee will stay at a company or leave (churn)\n", "        - Whether a tumor is cancerous or benign\n", "        - Whether a flower is a rose, a dandelion, a tulip, or a daffodil\n", "        - Whether a voter is Republican, Democrat, or Independent\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check out the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets.php?format=&task=cla&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table), and browse there classification datasets.  Which one's catch your eye?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### We are still dealing with **labeled data**.\n", "\n", "![labels](https://media.giphy.com/media/26Ff5evMweBsENWqk/giphy.gif)\n", "\n", "\n", "This is still supervised learning. \n", "\n", "But now, instead of the label being a continuous value, such as house price, the label is the category.  This can be either binary or multiclass.  But we still need the **labels** to train our models.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Compare linear to logistic regression as classification models\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The goal of logistic regression, and any classification problem, is to build a model which accurately separates the classes based on independent variables.  \n", "\n", "We are already familiar with how linear regression finds a best-fit \"line\".  It uses the **MSE cost function** to minimize the difference between true and predicted values.  \n", "\n", "A natural thought would be to use that \"line\" to discriminate between classes: Everything with an output greater than a certain point is classified as a 1, everything below is classified as a 0.\n", "\n", "Logistic regression does just this, but with some fancy tricks. \n", "\n", "The logistic classifer is **parametric, discriminitive** function.  The best fit parameters ($\\beta$)s creates a decision boundary which allows us to discriminate between the classes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://www.researchgate.net/publication/325813999/figure/fig5/AS:638669773893635@1529282148432/Classification-decision-boundary-using-logistic-regression-The-blue-area-corresponds-to.png\" style=\"height:500px;\">\n", "\n", "<center><small><a src=\"https://www.researchgate.net/publication/325813999/figure/fig5/AS:638669773893635@1529282148432/Classification-decision-boundary-using-logistic-regression-The-blue-area-corresponds-to.png\">img source </a></small></center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Breast Cancer Dataset\n", "\n", "Logistic regression \"is widely used in biostatistical applications where binary responses (two classes) occur quite frequently. For example, patients survive\n", "or die, have heart disease or not, or a condition is present or absent.\"   [Elements of Statistical Learning, Ch. 4, p. 119](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["[data_source](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://storage.googleapis.com/kaggle-datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-card.jpg\" style=\"height:300px;\">"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:12:44.128178Z", "start_time": "2021-04-06T20:12:43.790369Z"}}, "outputs": [], "source": ["# Breast Cancer identification dataset\n", "df = pd.read_csv('data/breast_cancer.csv', index_col= 0)\n", "\n", "df.drop(columns= ['Unnamed: 32'], inplace = True)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have 30 predictor columns, and 1 target column.  Our target column, however, is not in a form suitable for classification.  \n", "\n", "Assumption: **Binary logistic** regression requires the dependent variable to be binary.\n", "\n", "We will define malignant as our positive \"1\" class, and Benign as our \"0\" class."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Individual Exercise (turn off camera, take 2 minutes)\n", "\n", "1. Transform the target to a binary column using an sklearn transformer \n", "2. Use 'area_mean' as the independent variable and the transformed target as the dependent variable and apply a linear regression model to this dataset.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:12:46.234694Z", "start_time": "2021-04-06T20:12:45.582003Z"}}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:13:02.457139Z", "start_time": "2021-04-06T20:13:02.210685Z"}}, "outputs": [], "source": ["# Plot the resulting regression line.\n", "fig, ax = plt.subplots(figsize=(7,7))\n", "sns.scatterplot(df.area_mean, df.Target, hue=df.Target)\n", "sns.scatterplot(df.area_mean, list(y_hat),color='black')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "- According to the linear regression model, what would be your prediction if area_mean = 350?\n", "\n", "- What about if 'area_mean' is 5?\n", "\n", "- What about 2000?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Those predictions are not within the bounds of our target's sample space. In fact, linear regression could produce predictions from **-$\\infty$ to $\\infty$**  \n", "\n", "\n", "In order to fix that, we can set a threshold which determines a 0 or 1 value.\n", "Let's set the threshhold at .5."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:13:23.710899Z", "start_time": "2021-04-06T20:13:23.707527Z"}}, "outputs": [], "source": ["lr_prd = (y_hat > .5).astype(int)\n", "df['lr_yhat'] = lr_prd"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:13:29.233801Z", "start_time": "2021-04-06T20:13:29.001232Z"}}, "outputs": [], "source": ["# Now, let's plot again\n", "fig, ax = plt.subplots(figsize=(7,7))\n", "sns.scatterplot(df.area_mean, df.Target, hue=df.Target)\n", "sns.scatterplot(df.area_mean, list(y_hat), hue=df.lr_yhat)\n", "ax.hlines(.5, xmin=0, xmax=2500)\n", "ax.get_legend().remove();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at how many predictions linear regression got wrong."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:13:34.288222Z", "start_time": "2021-04-06T20:13:34.284545Z"}}, "outputs": [], "source": ["print(f\"Linear Regression missed {(df.Target != df.lr_yhat).sum()} predictions\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The **confusion matrix** will be an important visualization in classification. It will allow us to see the distribution of prediction results. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:13:46.350113Z", "start_time": "2021-04-06T20:13:46.343654Z"}}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\n", "\n", "cm_lin = confusion_matrix(df.Target, df.lr_yhat)\n", "cm_lin"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:14:10.403096Z", "start_time": "2021-04-06T20:14:10.260759Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(6,6))\n", "\n", "sns.heatmap(cm_lin, annot=True, ax=ax, square=True, cmap=\"coolwarm\", fmt='g', xticklabels=['B', 'M'],yticklabels=['B', 'M'] )\n", "plt.rcParams.update({'font.size': 30})\n", "ax.set_xlabel('Predicted', fontdict={'size': 15})\n", "ax.set_ylabel('True', fontdict={'size': 15})\n", "ax.set_title('Linear Regression\\n Tumor Diagnostic Predictions', fontdict={'size': 15} )\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Now Let's Try Logistic Regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:20:46.368192Z", "start_time": "2021-04-06T20:20:46.357631Z"}}, "outputs": [], "source": ["# Note: Same module as Linear Regression\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "# Same process: Instantiate instance of the algorithm\n", "log_reg = LogisticRegression()\n", "\n", "# Fit to data\n", "log_reg.fit(df[['area_mean']], df['Target'])\n", "\n", "# Predict \n", "yhat_log = log_reg.predict(df[['area_mean']])\n", "\n", "# predict_proba: This is new!\n", "yhat_log_proba = log_reg.predict_proba(df[['area_mean']])\n", "\n", "df['yhat_log_proba'] = yhat_log_proba[:,1]\n", "df['yhat_log'] = yhat_log"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:24:09.859734Z", "start_time": "2021-04-06T20:24:09.633264Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(6,6))\n", "sns.scatterplot(df.area_mean, df.Target, color='gray')\n", "sns.scatterplot(df.area_mean, df.yhat_log_proba, hue=df.yhat_log)\n", "ax.hlines(.5, xmin=0, xmax=2500)\n", "ax.get_legend().remove()\n", "ax.set_title('Logistic Regression Predictions')\n", "plt.style.use(['default']);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Look at that nice S-shape that fits our data so much more naturally."]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Your Turn__\n", "\n", "Use the trained logistic regression and make predictions for \n", "\n", "- area_mean = 350\n", "- area_mean = 5\n", "- area_mean = 2000"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Logistic regression's predict function automatically converts the predicted probabilities to categorical predctions.\n", "To return the probabilities, use the `predict_proba` method."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T19:03:34.088442Z", "start_time": "2021-04-06T19:03:34.084135Z"}}, "outputs": [], "source": ["log_reg.predict_proba([[5], [350],[2000]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How did our logistic regression model compare with our linear regression?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T19:03:46.778323Z", "start_time": "2021-04-06T19:03:46.774933Z"}}, "outputs": [], "source": ["print(f\"Logistic Regression missed {(df.Target != df.yhat_log).sum()} predictions\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:24:53.640269Z", "start_time": "2021-04-06T21:24:53.627656Z"}}, "outputs": [], "source": ["# Let's look at how many differences there were between our two predictions\n", "print('Predictions that were different between Linear and Logistic:',(df.lr_yhat != df.yhat_log).sum())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:27:13.545573Z", "start_time": "2021-04-06T20:27:13.540874Z"}}, "outputs": [], "source": ["lr_logr_diffs = df[(df.lr_yhat != df.yhat_log) & (df.yhat_log == df.Target)]\n", "print('Predictions that Linear got wrong and Logistic got right:',lr_logr_diffs.shape[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:28:17.516108Z", "start_time": "2021-04-06T20:28:17.303611Z"}}, "outputs": [], "source": ["fig, (ax1, ax2) = plt.subplots(1,2,figsize=(6,6))\n", "\n", "# Linear Regression Confusion Matrix\n", "sns.heatmap(cm_lin, annot=True, ax=ax1,cbar=False, square=True, \n", "            cmap=\"coolwarm\", fmt='g', xticklabels=['B', 'M'],yticklabels=['B', 'M'] )\n", "\n", "ax1.set_xlabel('Predicted', fontdict={'size': 15})\n", "ax1.set_ylabel('True', fontdict={'size': 15})\n", "ax1.set_title('Linear Regression', fontdict={'size': 15} )\n", "\n", "\n", "# Logistic Regression Confusion Matrix\n", "cm_log = confusion_matrix(df.Target, df.yhat_log)\n", "sns.heatmap(cm_log, annot=True, ax=ax2,  square=True, \n", "            cbar=False, cmap=\"coolwarm\", fmt='g',\n", "            xticklabels=['B', 'M'],yticklabels=['', ''] )\n", "\n", "ax2.set_xlabel('Predicted', fontdict={'size': 15})\n", "\n", "ax2.set_title('Logistic Regression', fontdict={'size': 15} )\n", "plt.show()\n", "plt.tight_layout();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Threshold"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By default, the predict() method applies a threshold of .05 to our prediction probabilities."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T19:05:28.060035Z", "start_time": "2021-04-06T19:05:28.051708Z"}}, "outputs": [], "source": ["# We see that converting all values of the column associated with the probability of a class 1 (Malignant)\n", "# is equal to the output of the predict() method\n", "(log_reg.predict_proba(df[['area_mean']])[:,1] > .5).astype(int) == log_reg.predict(df[['area_mean']])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["However, we may want to be more conservative in our estimate. With medical diagnostics in particular, certain errors are more important to catch.\n", "\n", "Which errors have particularly negative consequences in the scenario above?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["To err on the side of caution, we can force our model to predict more conservatively.  \n", "\n", "By lowering the threshold from .5, our model will predict more positive values, thereby decreasing our false negatives.  Consequently, our false positive rate will go up."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Code here\n", "yhat_lower_thresh = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:31:54.494726Z", "start_time": "2021-04-06T20:31:54.490403Z"}}, "outputs": [], "source": ["cm_lower_thresh = confusion_matrix(df.Target, yhat_lower_thresh)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:32:12.235077Z", "start_time": "2021-04-06T20:32:12.066775Z"}}, "outputs": [], "source": ["fig, (ax1, ax2) = plt.subplots(1,2,figsize=(6,6))\n", "\n", "# Linear Regression Confusion Matrix\n", "sns.heatmap(cm_log, annot=True, ax=ax1,cbar=False, square=True, \n", "            cmap=\"coolwarm\", fmt='g', xticklabels=['B', 'M'],yticklabels=['B', 'M'] )\n", "plt.rcParams.update({'font.size': 30})\n", "ax1.set_xlabel('Predicted', fontdict={'size': 15})\n", "ax1.set_ylabel('True', fontdict={'size': 15})\n", "ax1.set_title('Threshold .5', fontdict={'size': 15} )\n", "\n", "\n", "# Logistic Regression Confusion Matrix\n", "cm_log = confusion_matrix(df.Target, df.yhat_log)\n", "sns.heatmap(cm_lower_thresh, annot=True, ax=ax2,  square=True, \n", "            cbar=False, cmap=\"coolwarm\", fmt='g',\n", "            xticklabels=['B', 'M'],yticklabels=['', ''] )\n", "\n", "ax2.set_xlabel('Predicted', fontdict={'size': 15})\n", "\n", "ax2.set_title('Threshold .4', fontdict={'size': 15} )\n", "plt.show()\n", "plt.tight_layout();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Logistic Regression Under the Hood"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As we have seen above, linear regression outputs a value that can range anywhere from $-\\infty$ to $\\infty$.  \n", "\n", "Logistic regression attempts to convert those linear outputs to a range of probabilities, i.e. a value between 0 and 1.\n", "\n", "To make this conversion, we use the sigmoid function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![sigmoid](https://media.giphy.com/media/GtKtQ9Gb064uY/giphy.gif)\n", "\n", "\n", "<img src='https://cdn-images-1.medium.com/max/1600/1*RqXFpiNGwdiKBWyLJc_E7g.png' />\n", "\n", "As \u2018t\u2019 goes to infinity, Y(predicted) will inch closer to 1, and as \u2018t\u2019 goes to negative infinity, Y(predicted) will inch closer to 0.\n", "\n", "Using the sigmoid function above, if t = 1, the estimated probability would around .7. This tells that there is 70% chance that this observation would fall in the positive class.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's code out the sigmoid:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:34:32.911281Z", "start_time": "2021-04-06T20:34:32.907167Z"}}, "outputs": [], "source": ["# Outputs of sigmoid function\n", "print(sigmoid(0))\n", "print(sigmoid(10))\n", "print(sigmoid(-10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we substitute the product of our linear equation for x in the function above, and rephrase the objective of logistic regression as computing the probability of a class (assume positive class 1) given a set of $\\beta$ parameters, our formula becomes:\n", "\n", "$$\\Large P(Class = 1|X = x) =  \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}} $$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Some arithmetic (see appendix) allows us to see what the linear equation represents in our logistic regression:\n", "<br><br>\n", "    $\\ln{\\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)}} = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2...\\beta_n*X_n$\n", "    \n", "\n", "Our linear function calculates the log of the probability we predict 1, divided by the probability of predicting 0.  In other words, the linear equation is calculating the **log of the odds** that we predict a class of 1.\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Let's produce the log odds for the fourth observation in our dataset**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:34:53.347127Z", "start_time": "2021-04-06T20:34:53.343545Z"}}, "outputs": [], "source": ["log_odds_sample_4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, apply the sigmoid function above to convert the log-odds back to a probability."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check out work with sklearn's `predict_proba` method. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:35:59.128028Z", "start_time": "2021-04-06T20:35:59.123468Z"}}, "outputs": [], "source": ["log_reg.predict_proba([[df.area_mean.iloc[3]]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Interpreting Logistic Regression Coefficients\n", "\n", "[This ucla article is an excellent resource](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)\n", "\n", "To interpet the coefficients:\n", "1. Use the coefficient as the exponent to *e* `np.exp(coefficient)`\n", "2. If the ones place contains a 1, then you can interpret that coefficient by dropping everying before the decimal place and moving the decimal two places to the right. This becomes the percentage change of the odds for a positive result with a 1 unit increase in the independent variable.\n", "    - If the ones place contains a 2, then you add 100 to the percentage, resulting in a `100%+` change in odds. \n", "The TL;DR is  can interpret coefficients, as: \"With a 1 unit increase in the independent variable, you would expect to see a "]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we have a coefficient of .1229589, we would do the following:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:53:43.044084Z", "start_time": "2021-04-06T20:53:43.040096Z"}}, "outputs": [], "source": ["np.exp(.1229589)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Based on this output, we would describe the relationship as:\n", "> \"With a 1 unit increase in the independent variable, we would expect to see a 13% increase in the odds of a positive result\"\n", "\n", "Interpreting our model coefficient:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:39:42.577323Z", "start_time": "2021-04-06T20:39:42.573485Z"}}, "outputs": [], "source": ["log_reg.coef_[0][0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:59:27.686741Z", "start_time": "2021-04-06T20:59:27.682905Z"}}, "outputs": [], "source": ["np.exp(0.00859938772943465)"]}, {"cell_type": "markdown", "metadata": {"ExecuteTime": {"end_time": "2021-04-06T20:50:45.768697Z", "start_time": "2021-04-06T20:50:45.765047Z"}}, "source": ["\"With a 1 unit increase in area mean, we would expect to see a .86% increase in the odds of a positive result\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 6. Hyperparameter Tuning the C Variable"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have discussed 'L1' (lasso)  and 'L2' (ridge) regularization.  If you looked at the docstring of Sklearn's Logistic Regression function, you may have noticed that we can specify different types of regularization when fitting the model via the `penalty` parameter.\n", "\n", "We can also specificy the strength of the regularization via the `C` parameter. `C` is the inverse regularization strength.  So, a low `C` means high regularization strength."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's run through our train test split process, and tune our C parameter.   "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:29:41.835169Z", "start_time": "2021-04-06T21:29:41.831626Z"}}, "outputs": [], "source": ["y = df.Target\n", "X = df.drop([\"Target\", \"diagnosis\"], axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:00:30.082727Z", "start_time": "2021-04-06T21:00:30.077285Z"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scaling is important when implementing regularization, since it penalizes the magnitude of the coefficients.\n", "\n", "To correctly implement scaling, we scale only on the training data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pair Annotation\n", "\n", "With a partner, put annotations after the empty # comments in the KFold implementation below."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:03:40.958418Z", "start_time": "2021-04-06T21:03:31.998626Z"}}, "outputs": [], "source": ["from sklearn.model_selection import KFold \n", "from sklearn.preprocessing import StandardScaler\n", "\n", "c_accuracy = {}\n", "\n", "#\n", "kf = KFold(n_splits=4)\n", "\n", "#\n", "cs = np.linspace(1,11,100)\n", "for c in cs:\n", "    \n", "    mean_accuracy = []\n", "    \n", "    #\n", "    for train_ind, val_ind in kf.split(X_train, y_train):\n", "        \n", "        #\n", "        X_tt, y_tt = X_train.iloc[train_ind], y_train.iloc[train_ind]\n", "        X_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind]\n", "        \n", "        ss = StandardScaler()\n", "        \n", "        #\n", "        X_tt = ss.fit_transform(X_tt)\n", "        X_val = ss.transform(X_val)\n", "        \n", "        #\n", "        log_reg = LogisticRegression(C=c, solver='lbfgs', max_iter=400)\n", "        \n", "        log_reg.fit(X_tt, y_tt)\n", "        \n", "        mean_accuracy.append(log_reg.score(X_val, y_val))\n", "    \n", "    # \n", "    c_accuracy[c] = np.mean(mean_accuracy)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:09:09.513385Z", "start_time": "2021-04-06T21:09:09.508830Z"}}, "outputs": [], "source": ["sorted(c_accuracy.items(), key=lambda kv: kv[1], reverse=True)[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Now that we have selected a C hyperparameter that performs well, fit to the entire training set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:14:16.295851Z", "start_time": "2021-04-06T21:14:16.280396Z"}}, "outputs": [], "source": ["ss = StandardScaler()\n", "\n", "X_train_sc = ss.fit_transform(X_train)\n", "log_reg = LogisticRegression(C=2, solver='lbfgs', max_iter=5000)\n", "log_reg.fit(X_train_sc, y_train)\n", "\n", "y_hat = log_reg.predict(X_train_sc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:14:16.719550Z", "start_time": "2021-04-06T21:14:16.622954Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "cm = confusion_matrix(y_train, y_hat)\n", "sns.heatmap(cm, ax=ax, annot=True,  square=True, \n", "            cbar=False, cmap=\"coolwarm\", fmt='g',\n", "            xticklabels=['B', 'M'],yticklabels=['B', 'M'] )\n", "\n", "ax.set_xlabel('Predicted', fontdict={'size': 15})\n", "ax.set_ylabel('True', fontdict={'size': 15})\n", "ax.set_title('Logistic Regression: C2', fontdict={'size': 15})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can adjust the threshold to catch more false positives."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:11:37.817598Z", "start_time": "2021-04-06T21:11:37.814355Z"}}, "outputs": [], "source": ["y_hat = (log_reg.predict_proba(X_train_sc)[:,1] >.4).astype(int)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:11:38.442346Z", "start_time": "2021-04-06T21:11:38.340857Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "cm = confusion_matrix(y_train, y_hat)\n", "sns.heatmap(cm, ax=ax, annot=True,  square=True, \n", "            cbar=False, cmap=\"coolwarm\", fmt='g',\n", "            xticklabels=['B', 'M'],yticklabels=['B', 'M'] )\n", "\n", "ax.set_xlabel('Predicted', fontdict={'size': 15})\n", "ax.set_ylabel('True', fontdict={'size': 15})\n", "ax.set_title('Logistic Regression: Threshold .4', fontdict={'size': 15})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Now apply to the test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:11:44.337121Z", "start_time": "2021-04-06T21:11:44.333734Z"}}, "outputs": [], "source": ["X_test_sc = ss.transform(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:11:44.673946Z", "start_time": "2021-04-06T21:11:44.670797Z"}}, "outputs": [], "source": ["y_hat = (log_reg.predict_proba(X_test_sc)[:,1] >.4).astype(int)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:11:45.087845Z", "start_time": "2021-04-06T21:11:44.988388Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "cm = confusion_matrix(y_test, y_hat)\n", "sns.heatmap(cm, ax=ax, annot=True,  square=True, \n", "            cbar=False, cmap=\"coolwarm\", fmt='g',\n", "            xticklabels=['B', 'M'],yticklabels=['B', 'M'] )\n", "\n", "ax.set_xlabel('Predicted', fontdict={'size': 15})\n", "ax.set_ylabel('True', fontdict={'size': 15});\n", "ax.set_title('Holdout Test Results', fontdict={'size': 15})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our logistic regression coefficients, we can inspect which features our model thinks are most important for the different classifications."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:12:02.744378Z", "start_time": "2021-04-06T21:12:02.739368Z"}}, "outputs": [], "source": ["beta_values = {name:coef for name, coef in zip(list(X_test.columns), list(log_reg.coef_[0]))}\n", "col = [item[0] for item in sorted(beta_values.items(), key= lambda kv: kv[1], reverse=True)]\n", "beta = [item[1] for item in sorted(beta_values.items(), key= lambda kv: kv[1], reverse=True)]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-06T21:12:22.543964Z", "start_time": "2021-04-06T21:12:22.249354Z"}}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7,7))\n", "\n", "plt.rcParams.update({'font.size': 20})\n", "\n", "bars = ax.barh(col,beta, color='g')\n", "  \n", "for bar in range(1,8):\n", "    bars[-bar].set_color('red')\n", "\n", "ax.set_title('Relative Importance of Features');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 8. Assumptions of Logistic Regression\n", "\n", "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms \u2013 particularly regarding linearity, normality, and homoscedasticity.\n", "\n", "First, logistic regression does not require a linear relationship between the dependent and independent variables.  Second, the error terms (residuals) do not need to be normally distributed.  Third, homoscedasticity is not required.  \n", "\n", "**The following assumptions still apply:**\n", "\n", "1.  Binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n", "\n", "2. Logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n", "\n", "3. Logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n", "\n", "4. Logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n", "\n", "5. Logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Appendix: Converting sigmoid to log-odds."]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we substitute the product of our linear equation for x in the function above, and rephrase the objective of logistic regression as computing the probability of a class (assume positive class 1) given a set of $\\beta$ parameters, our formula becomes:\n", "\n", "$$\\Large P(Class = 1|X = x) =  \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}} $$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, with some arithmetic:\n", "\n", "You can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}$\n", "\n", "\n", "$$ \\Large P(G = 1|X = x) = \\displaystyle \\frac{e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}$$\n", "\n", "As a result, you can compute:\n", "\n", "$$ \\Large P(G = 0|X =x) = 1- \\displaystyle \\frac{e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}$$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Further:\n", "\n", "$$ \\Large \\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)} = e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n} $$\n", "\n", "This expression can be interpreted as the *odds in favor of class 1*.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probability and odds\n", "\n", "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n", "\n", "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n", "\n", "Examples:\n", "\n", "- Dice roll of 1: probability = 1/6, odds = 1/5\n", "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n", "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n", "\n", "$$odds = \\frac {probability} {1 - probability}$$\n", "\n", "$$probability = \\frac {odds} {1 + odds}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This expression can be interpreted as the *odds in favor of class 1*.  \n", "\n", "$$ \\Large \\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)} = e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n} $$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, taking the log of both sides leads to:\n", "<br><br>\n", "    $\\ln{\\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)}} = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2...\\beta_n*X_n$\n", "    \n", "Here me can see why we call it logisitic regression.\n", "\n", "Our linear function calculates the log of the probability we predict 1, divided by the probability of predicting 0.  In other words, the linear equation is calculating the **log of the odds** that we predict a class of 1.\n", "    "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 4}