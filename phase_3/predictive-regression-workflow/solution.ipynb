{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Regression Modeling Workflow\n",
    "\n",
    "## Students Will Be Able To\n",
    " - Understand the basic outline of a predictive modeling workflow\n",
    " - Describe the similarities and differences between this predictive regression workflow and the previous (Phase 2) inferential regression workflow\n",
    "\n",
    "## Business Understanding and Data Understanding\n",
    "\n",
    "This dataset was downloaded from [Kaggle](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho) and contains information about **used car sale listings**.  We are trying to **predict the price** associated with the listing.\n",
    "\n",
    "### Features (as described on Kaggle)\n",
    " - `Car_Name`: The name of the car\n",
    " - `Year`: The year in which the car was bought\n",
    " - `Selling_Price`: The price the owner wants to sell the car at\n",
    " - `Present_Price`: The current ex-showroom price of the car\n",
    " - `Kms_Driven`: The distance completed by the car in km\n",
    " - `Fuel_Type`: The fuel type of the car (`Petrol`, `Diesel`, or Other)\n",
    " - `Seller_Type`: Whether the seller is a dealer or an individual\n",
    " - `Transmission`: Whether the car is manual or automatic\n",
    " - `Owner`: The number of owners the car has previously had\n",
    "\n",
    "Looking at the original website, it looks like the prices are listed in lakhs, meaning hundreds of thousands of rupees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: Business Understanding and Data Understanding*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:29:52.253571Z",
     "start_time": "2021-04-01T19:29:52.250520Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostly the same...looked at what features we have, missing values, descriptive statistics\n",
    "plots\n",
    "\n",
    "Difference: we're trying to predict price, we don't necessarily care about the relationship\n",
    "between the features and the target\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Before performing any preprocessing or modeling, set aside a holdout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Selling_Price\", axis=1)\n",
    "y = df[\"Selling_Price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: Train-Test Split*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:29:53.637559Z",
     "start_time": "2021-04-01T19:29:53.634170Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Difference: train and test sets separate\n",
    "\n",
    "Similarity: identify, separate out the target variable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Dummy Model with Numeric Features Only\n",
    "\n",
    "We have four numeric features (`Year`, `Present_Price`, `Kms_Driven`, and `Owner`) and four non-numeric features (`Car_Name`, `Fuel_Type`, `Seller_Type`, `Transmission`).  Before doing any of the engineering work to be able to use those non-numeric features, let's just try using the numeric ones\n",
    "\n",
    "\n",
    "[Documentation for Sklearn's DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = DummyRegressor(strategy='median')\n",
    "X_train_numeric = X_train[[\"Year\", \"Present_Price\", \"Kms_Driven\", \"Owner\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cross_val_score = cross_val_score(baseline, X_train_numeric, y_train, cv=5)\n",
    "baseline_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: Baseline Model*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:35:17.163729Z",
     "start_time": "2021-04-01T19:35:17.160469Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Differences:\n",
    "- Not using all the data, just using training data\n",
    "- Using cross validation, which we didn't do before\n",
    "- Model evaluation is just r-squared\n",
    "- Didn't check assumptions\n",
    "- Not using StatsModels, using SciKit-Learn\n",
    "- Used a dummy model first\n",
    "\n",
    "Similarities:\n",
    "- Using r-squared to evaluate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cross_val_score = cross_val_score(lin_reg_model, X_train_numeric, y_train, cv=5)\n",
    "baseline_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too bad, we are getting somewhere between 0.67 and 0.89 r-squared for a linear regression with just the numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add One-Hot Encoded Features\n",
    "\n",
    "Let's see if adding in some of those non-numeric features helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_features = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_concat_feature_train(X_train_all_features, feature_name):\n",
    "    \"\"\"\n",
    "    Helper function for transforming training data.  It takes in the full X dataframe and\n",
    "    feature name, makes a one-hot encoder, and returns the encoder as well as the dataframe\n",
    "    with that feature transformed into multiple columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    \n",
    "    # make a one-hot encoder and fit it to the training data\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False)\n",
    "    single_feature_df = X_train_all_features[[feature_name]]\n",
    "    ohe.fit(single_feature_df)\n",
    "    \n",
    "    # call helper function that actually encodes the feature and concats it\n",
    "    X_train_all_features = encode_and_concat_feature(X_train_all_features, feature_name, ohe)\n",
    "    \n",
    "    return ohe, X_train_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_concat_feature(X, feature_name, ohe):\n",
    "    \"\"\"\n",
    "    Helper function for transforming a feature into multiple columns of 1s and 0s. Used\n",
    "    in both training and testing steps.  Takes in the full X dataframe, feature name, \n",
    "    and encoder, and returns the dataframe with that feature transformed into multiple\n",
    "    columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # create new one-hot encoded df based on the feature\n",
    "    single_feature_df = X[[feature_name]]\n",
    "    feature_array = ohe.transform(single_feature_df)\n",
    "    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0], index=single_feature_df.index)\n",
    "    \n",
    "    # drop the old feature from X and concat the new one-hot encoded df\n",
    "    X.drop(feature_name, axis=1, inplace=True)\n",
    "    X = pd.concat([X, ohe_df], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need each of these encoders later for transforming the test data\n",
    "\n",
    "fuel_type_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Fuel_Type\")\n",
    "seller_type_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Seller_Type\")\n",
    "transmission_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Transmission\")\n",
    "# putting car name at the end just because it has the most categories\n",
    "car_name_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Car_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: One-Hot Encoding*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:45:35.994280Z",
     "start_time": "2021-04-01T20:45:35.990978Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Similarity:\n",
    "- Dummying out categorical variables (our model still needs 100% numbers)\n",
    "\n",
    "Differences:\n",
    "- Didn't drop any columns when dummying categorical variables (not worried about multicollinearity)\n",
    "- We added everything, didn't do much EDA beforehand or add features one at a time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "print(\"Old:\", baseline_cross_val_score)\n",
    "print(\"New:\", cross_val_score(lin_reg_model, X_train_all_features, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks worse.  What if we don't use the car name, and just use the categories with 1-3 values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_except_car_name = X_train_all_features[[\n",
    "                    \"Year\",\n",
    "                    \"Present_Price\",\n",
    "                    \"Kms_Driven\",\n",
    "                    \"Owner\",\n",
    "                    \"CNG\",\n",
    "                    \"Diesel\",\n",
    "                    \"Petrol\",\n",
    "                    \"Dealer\",\n",
    "                    \"Individual\",\n",
    "                    \"Automatic\",\n",
    "                    \"Manual\"\n",
    "                ]].copy()\n",
    "X_train_all_except_car_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "print(\"Old:\", baseline_cross_val_score)\n",
    "print(\"New:\", cross_val_score(lin_reg_model, X_train_all_except_car_name, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, adding these categories improved r-squared for 4 out of 5 subsamples compared to just having numeric features, so let's keep them for our linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_linreg_cross_val_score = cross_val_score(lin_reg_model, X_train_all_except_car_name, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: Linear Regression with More Features*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Differences:\n",
    "- Just compared r-squared\n",
    "- Used cross validation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a More Advanced Model\n",
    "\n",
    "It depends on our business case whether these numbers are sufficient.  We are explaining approximately somewhere between 65% and 92% of the variance in the sale price.  But let's try a more complicated model.\n",
    "\n",
    "First, just using the X_train values used in the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_1 = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "print(\"Old:\", best_linreg_cross_val_score)\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this more-sophisticated model is performing slightly better on 4 of 5 subsamples than the best linear regression score.  Let's see what happens if we add the car names back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_2 = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "print(\"Old:\", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train, cv=5))\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_2, X_train_all_features, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one of the subsamples improved with adding this feature, and everything else got worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning the More Advanced Model\n",
    "\n",
    "Let's add some more \"power\" to the random forest regressor, since it's running reasonably quickly right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "print(\"Old:\", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train, cv=5))\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That marginally improved 4 of the 5 subsamples (but was significantly slower to run).  Let's try including the car name again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "print(\"Old:\", cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train, cv=5))\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_4, X_train_all_features, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, that didn't really seem to help.  So if we're stopping right now, we can say that the third random forest regressor is the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: Hyperparameter Tuning*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:14.100731Z",
     "start_time": "2021-04-01T20:50:14.097401Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Similar:\n",
    "- Adding more features isn't always better\n",
    "- Sometimes the last model you build isn't the best\n",
    "- We're iteratively building models until we run out of time, then picked the \"best\"\n",
    "\n",
    "Different:\n",
    "- Before, didn't have hyperparameters to tune, now we do\n",
    "- Computational complexity is a concern\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that we have chosen a best model, let's use the holdout set to see how well the final model does\n",
    "\n",
    "### Preprocessing to Use Test Data\n",
    "First, perform all of the same transformations on the test X that were performed on the train X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all_except_car_name = X_test.reset_index().drop([\"index\", \"Car_Name\"], axis=1)\n",
    "\n",
    "# fuel_type_ohe, seller_type_ohe, and transmission_ohe were fitted on the training data\n",
    "X_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, \"Fuel_Type\", fuel_type_ohe)\n",
    "X_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, \"Seller_Type\", seller_type_ohe)\n",
    "X_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, \"Transmission\", transmission_ohe)\n",
    "\n",
    "X_test_all_except_car_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Scoring Final Model\n",
    "\n",
    "Fit our best model on all of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3.fit(X_train_all_except_car_name, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score our best model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3.score(X_test_all_except_car_name, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good!  We have a model that is able to explain 97% of the variance in the car sale list prices\n",
    "\n",
    "### Fitting and Scoring Baseline Model\n",
    "\n",
    "Let's compare that to the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model.fit(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numeric = X_test[[\"Year\", \"Present_Price\", \"Kms_Driven\", \"Owner\"]].copy()\n",
    "\n",
    "lin_reg_model.score(X_test_numeric, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our r-squared has improved from a baseline of 85% to 97%.\n",
    "\n",
    "### Other Metrics\n",
    "\n",
    "To report something more applicable to a business audience, let's calculate the [root mean squared error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) using the [metrics submodule of scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:10:15.998567Z",
     "start_time": "2021-04-01T21:10:15.938847Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, random_forest_regressor_model_3.predict(X_test_all_except_car_name))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret this: on average, our prediction of `Selling_Price` is off (either too high or too low) by about 0.9 lakh, i.e. about 90,000 rupees (about 1200 USD)\n",
    "\n",
    "Let's compare that to the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mse = mean_squared_error(y_test, lin_reg_model.predict(X_test_numeric))\n",
    "np.sqrt(baseline_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the baseline model was off by about 2 lakh, i.e. about 200,000 rupees (about 2600 USD)\n",
    "\n",
    "### Visualization of Model Performance\n",
    "\n",
    "Also, here is a plot that shows the actual vs. predicted prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Customize which model is actually being plotted\n",
    "ax1.set_title(\"Baseline Regression Model Performance\")\n",
    "ax1.scatter(y_test, lin_reg_model.predict(X_test_numeric),\n",
    "                alpha=0.5, label=\"model output\", color=\"green\")\n",
    "\n",
    "ax2.set_title(\"Final Regression Model Performance\")\n",
    "ax2.scatter(y_test, random_forest_regressor_model_3.predict(X_test_all_except_car_name),\n",
    "                alpha=0.5, label=\"model output\")\n",
    "\n",
    "# Same setup for both plots (x and y labels, line showing y=x)\n",
    "y_equals_x = np.linspace(0, 25)\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel(\"True Price (Lakhs)\")\n",
    "    ax.set_ylabel(\"Predicted Price (Lakhs)\")\n",
    "    ax.plot(y_equals_x, y_equals_x, label=\"predicted = actual\", color=\"black\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Compare and Contrast: Model Evaluation*\n",
    "\n",
    "What (if anything) was similar between this process and the previous process?  What (if anything) was different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Differences:\n",
    "- Before we had a scatter plot with residuals, looking for heteroskedasticity\n",
    "  Now we are just comparing the predicted vs. actual values\n",
    "- No linear regression assumption check visualizations (or statistics), so no\n",
    "  Q-Q plot\n",
    "- Final evaluation that is different from what we were doing along the way\n",
    "  - \"Along the way\" we didn't use the test data, we used cross-validation instead\n",
    "  - At the end, we did use test data, didn't use cross-validation\n",
    "  - The goal is to avoid \"manually overfitting\" by choosing based on test set performance\n",
    "\n",
    "Similarities:\n",
    "- Comparison to a baseline (showing improvement)\n",
    "\n",
    "Overall process differences:\n",
    "- We didn't stop to evaluate after adding each individual feature\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1\n",
    "\n",
    "Quick note about `RandomForestRegressor` and why it \"performed better on the test than on the train\".  This isn't quite a correct characterization, but it's worth thinking about\n",
    "\n",
    "Let's look at the scores again from when we were choosing a \"best model\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, the code for the final evaluation of the \"best model\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3.fit(X_train_all_except_car_name, y_train)\n",
    "random_forest_regressor_model_3.score(X_test_all_except_car_name, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's definitely true that the cross-validated scores range from 73% to 93%, whereas the final evaluation score is 97% â€” a good amount higher\n",
    "\n",
    "But the important thing to be clear on is that these are _both_ scores on \"test data\", just different test sets.  For the cross-validated scores, those are scores from training on a subset and then testing on a subset of the training data.  Then the final score is created by training on the whole training set and testing on the test set.\n",
    "\n",
    "If we scored the final model on the train set, it's actually even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3.score(X_train_all_except_car_name, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, to go back to the original question, why is the final test score so much better than the cross-validated test scores?\n",
    "\n",
    "First, a reminder that we are working with a pretty tiny dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 splits for cross-validation\n",
    "225 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 out of 5 splits used for training, remaining used for testing\n",
    "45 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing a 5-fold cross-validation, that means that the cross-validated scores are each being computed with:\n",
    "\n",
    " - 180 rows in the training set\n",
    " - 45 rows in the test set\n",
    "\n",
    "By comparison, our overall model evaluation is using all of `X`, which means:\n",
    "\n",
    " - 225 rows in the training set\n",
    " - 76 rows in the test set\n",
    "\n",
    "When we are working with such a small dataset, being able to train on 45 additional data points (and having the errors in the test data be spread out over 31 more data points) can naturally mean better performance.  It's typical for final results to be better than cross-validated results unless you have a very large dataset!\n",
    "\n",
    "Separately, we might have just randomly chosen a \"lucky\" test set, that includes fewer outliers on average, or is more easily predicted by the features we have (rather than some unknown features we don't have). Because we used cross-validation to choose our final model (rather than evaluating on the test set to choose our final model), this random concern could just as easily be an \"unlucky\" test set as a \"lucky\" one.  But if we had used the test set for choosing a model, we could run into issues where we systematically overestimate our model's predictive abilities, because we didn't actually evaluate on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
