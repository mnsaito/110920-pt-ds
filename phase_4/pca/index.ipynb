{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:52.673340Z",
     "start_time": "2021-05-11T16:47:51.442485Z"
    },
    "index": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 3
   },
   "source": [
    "Think of the predictors in your dataset as dimensions in what we can usefully call \"feature space\". If we're predicting house prices, then we might have a 'square feet' dimension or a 'number of bathrooms' dimension, etc. Then each record (of a house or a house sale, say) would be represented as a point (or vector) in this feature space. Some would score higher on the 'latitude' dimension or lower on the 'number of bedrooms' dimension, or whatever.\n",
    "\n",
    "One difficulty is that, despite our working nomenclature, these things aren't really *dimensions* in the truest sense, since they're not independent of each other. When we talk about the x-, y-, and z-dimensions of Euclidean 3-space, for example, one important feature is that values of x have no bearing (per se) on values of y or of z. I can move three units along the x-dimension without changing my y- or z-position.\n",
    "\n",
    "But the same thing is generally not true for datasets. When I increase my position along the 'number of bedrooms' dimension (or, better, *direction*), I also tend to increase my position along, say, the 'square feet' direction as well.\n",
    "\n",
    "This is problematic for a couple reasons: One is that my model could be in effect \"double-counting\" certain features of my signal, which can lead to overfit models. And if my goal is inference or explanation, then I'm going to have a very hard time distinguishing between the idea that the number of bedrooms is what's *really* predictive of housing prices and the idea that the number of square feet is what's really so predictive.\n",
    "\n",
    "Within a predictive lense, sometimes we may have a feature space that is so large (often as a product of OneHotEncoding) that there is no concievable way to produce a model that is not highly overfit to the training data. \n",
    "\n",
    "The idea behind Principal Component Analysis (PCA) is to transform our dataset into something more useful for building models. What we want to do is to build new dimensions (predictors) out of the dimensions we are given in such a way that:\n",
    "\n",
    "(1) each dimension we draw captures as much of the remaining variance among our predictors as possible; and <br/>\n",
    "(2) each dimension we draw is orthogonal to the ones we've already drawn.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Think back to multiple linear regression for a moment.\n",
    "\n",
    "The fundamental idea is that I can get a better prediction for my dependent variable by considering a *linear combination of my predictors* than I can get by considering any one predictor by itself.\n",
    "\n",
    "$\\rightarrow$ **PCA insight**: If the combinations of predictors work better than the predictors themselves, then let's just treat the combinations as our primary dimensions!\n",
    "\n",
    "But one problem with having lots of predictors is that it raises the chance that some will be nearly *collinear*.\n",
    "\n",
    "$\\rightarrow$ **PCA insight**: Since we're reconstructing our dimensions anyway, let's make sure that the dimensions we construct are mutually orthogonal! <br/>\n",
    "$\\rightarrow$ **PCA insight**: Moreover, since we'll be capturing much of the variance among our predictors in the first few dimensions we construct, we'll be able in effect to *reduce  the dimensionality* of our problem. Thus PCA is a fundamental tool in *dimensionality reduction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:52.680032Z",
     "start_time": "2021-05-11T16:47:52.674716Z"
    },
    "index": 4,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:52.750634Z",
     "start_time": "2021-05-11T16:47:52.736567Z"
    },
    "index": 6
   },
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:53.194434Z",
     "start_time": "2021-05-11T16:47:53.189757Z"
    },
    "index": 8
   },
   "outputs": [],
   "source": [
    "cars.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 10
   },
   "source": [
    "**Data Formatting**\n",
    "\n",
    "In the cell below, reformat the column names so \n",
    "- There are not preceeding or trailing spaces \n",
    "- All spaces and dashes have been replaced with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 11
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 13
   },
   "source": [
    "In the cell below, change `'cubicinches'` and `'weightlbs'` to a numeric datatype. Replace non convertable observations to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 16
   },
   "source": [
    "In the cell below, seperate `'mpg'` from the rest of the data, and create a train test split. \n",
    "- Assign the `'mpg'` column to the variable `y`.\n",
    "- Assign all other columns to the variable `X`.\n",
    "- Create a train test split with a `random_state` of 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 17
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:55.175921Z",
     "start_time": "2021-05-11T16:47:55.172390Z"
    },
    "index": 19
   },
   "outputs": [],
   "source": [
    "number_selector = make_column_selector(dtype_include='number')\n",
    "object_selector = make_column_selector(dtype_include='object')\n",
    "\n",
    "column_transform = make_column_transformer(\n",
    "                    (StandardScaler(), number_selector),\n",
    "                    (OneHotEncoder(), object_selector),\n",
    "                    remainder='passthrough')\n",
    "\n",
    "preprocessing = make_pipeline(column_transform, SimpleImputer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:55.679907Z",
     "start_time": "2021-05-11T16:47:55.659044Z"
    },
    "index": 21
   },
   "outputs": [],
   "source": [
    "preprocessing.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:56.182520Z",
     "start_time": "2021-05-11T16:47:56.170294Z"
    },
    "index": 23
   },
   "outputs": [],
   "source": [
    "X_tr_pp = preprocessing.transform(X_train)\n",
    "X_te_pp = preprocessing.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:56.685797Z",
     "start_time": "2021-05-11T16:47:56.674679Z"
    },
    "index": 25
   },
   "outputs": [],
   "source": [
    "# Let's construct a linear regression\n",
    "\n",
    "lr = LinearRegression().fit(X_tr_pp, y_train)\n",
    "\n",
    "# Score on train\n",
    "lr.score(X_tr_pp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:57.185115Z",
     "start_time": "2021-05-11T16:47:57.181260Z"
    },
    "index": 27
   },
   "outputs": [],
   "source": [
    "# Score on test\n",
    "\n",
    "lr.score(X_te_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:47:57.736227Z",
     "start_time": "2021-05-11T16:47:57.732564Z"
    },
    "index": 29
   },
   "outputs": [],
   "source": [
    "# Get the coefficients of the best-fit hyperplane\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 31
   },
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$- 1.555\\times cyl\\_sd + 2.189\\times in^3\\_sd - 1.154\\times hp\\_sd - 4.681\\times lbs.\\_sd  - 0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 32
   },
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "The key idea is to diagonalize (i.e. find the eigendecomposition of) the covariance matrix. The decomposition will produce a set of orthogonal vectors that explain as much of the remaining variance as possible. These are our [principal components](https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:18.004449Z",
     "start_time": "2021-05-11T16:48:18.000929Z"
    },
    "index": 33
   },
   "outputs": [],
   "source": [
    "matrix = np.array([[0,1], [1,0]])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:19.348814Z",
     "start_time": "2021-05-11T16:48:19.344762Z"
    },
    "index": 35
   },
   "outputs": [],
   "source": [
    "vector = [5,2]\n",
    "matrix.dot(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:20.334823Z",
     "start_time": "2021-05-11T16:48:20.330544Z"
    },
    "index": 37
   },
   "outputs": [],
   "source": [
    "np.linalg.eig(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:21.575289Z",
     "start_time": "2021-05-11T16:48:21.571366Z"
    },
    "index": 39
   },
   "outputs": [],
   "source": [
    "matrix.dot([0.70710678,0.70710678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 41
   },
   "source": [
    "The definition of an eigenvector is: $\\vec{x}$ is an eigenvector of the matrix $A$ if $A\\vec{x} = \\lambda\\vec{x}$, for some scalar $\\lambda$. That is, the vector is oriented in just such a direction that multiplying the matrix by it serves only to lengthen or shorten the original vector.\n",
    "\n",
    "Suppose we have the matrix\n",
    "$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "\n",
    "Let's calculate the eigendecomposition of this matrix.\n",
    "\n",
    "In order to do this, we set $(A - \\lambda I)\\vec{x} = 0$. One trivial solution is $\\vec{x} = \\vec{0}$, but if there are more interesting solutions, then it must be that $|A - \\lambda I| = 0$, which is to say that some column vector in $A - \\lambda I$ must be expressible as a linear combination of the other columns. (Otherwise, there would be no way to \"undo\" the multiplicative effect of a column vector on $\\vec{x}$!) For more on this point, see [this page](http://www2.math.uconn.edu/~troby/math2210f16/LT/sec1_7.pdf).\n",
    "\n",
    "So we have:\n",
    "\n",
    "$\\begin{vmatrix}\n",
    "a_{11} - \\lambda & a_{12} \\\\\n",
    "a_{21} & a_{22} - \\lambda\n",
    "\\end{vmatrix} = 0$\n",
    "\n",
    "$(a_{11} - \\lambda)(a_{22} - \\lambda) - a_{12}a_{21} = 0$\n",
    "\n",
    "$\\lambda^2 - (a_{11} + a_{22})\\lambda + a_{11}a_{22} - a_{12}a_{21}$\n",
    "\n",
    "$\\lambda = \\frac{a_{11} + a_{22}\\pm\\sqrt{(a_{11} + a_{22})^2 + 4(a_{12}a_{21} - a_{11}a_{22})}}{2}$\n",
    "\n",
    "Suppose e.g. we had\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "5 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "We can use the equation we just derived to solve for the eigenvalues of this matrix. Then we can plug *those* into our eigenvector definition to solve for the eigenvectors:\n",
    "\n",
    "So:\n",
    "\n",
    "### Eigenvalues\n",
    "\n",
    "$\\lambda = \\frac{5+5\\pm\\sqrt{(5+5)^2+4(3\\times 3 - 5\\times 5)}}{2} = 5\\pm\\frac{\\sqrt{36}}{2} = 2, 8$.\n",
    "\n",
    "### Eigenvectors\n",
    "\n",
    "Now we can plug those in. If we plug in $\\lambda = 8$, then we get:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "5-8 & 3 \\\\\n",
    "3 & 5-8\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-3 & 3 \\\\\n",
    "3 & -3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = 0.$\n",
    "\n",
    "So:\n",
    "\n",
    "$-3x_1 + 3x_2 = 0$ (or $3x_1 - 3x_2 = 0$)\n",
    "\n",
    "$x_1 = x_2$.\n",
    "\n",
    "Therefore, we find that any 2 element column vector in which the two elements have equal magnitude and the same sign are eigenvectors for this matrix.\n",
    "\n",
    "It is standard to scale eigenvectors to a magnitude of 1, and so we would write this eigenvector as\n",
    "$\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "If we plug in $\\lambda = 2$, we find a second eigenvector equal to\n",
    "$\\begin{bmatrix}\n",
    "-\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}$. Therefore, we find that any 2 element column vector in which the two elements have equal magnitude and opposite signs are eigenvectors for this matrix.\n",
    " (I'll leave this as an exercise.)\n",
    "\n",
    "**Thus we can express the full diagonalization of our matrix as follows**:\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "5 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "8 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2} \\\\\n",
    "-\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 42
   },
   "source": [
    "### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T21:19:54.013876Z",
     "start_time": "2021-05-11T21:19:53.929880Z"
    },
    "index": 43
   },
   "outputs": [],
   "source": [
    "# We can use np.linalg.eig()\n",
    "\n",
    "A = np.array([[5, 3], [3, 5]])\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:25.711982Z",
     "start_time": "2021-05-11T16:48:25.709487Z"
    },
    "index": 45
   },
   "outputs": [],
   "source": [
    "# np.linalg.eig(X) returns a double of NumPy arrays, the first containing\n",
    "# the eigenvalues of X and the second containing the eigenvectors of X.\n",
    "\n",
    "values, vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:26.209061Z",
     "start_time": "2021-05-11T16:48:26.205612Z"
    },
    "index": 47
   },
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:26.622523Z",
     "start_time": "2021-05-11T16:48:26.619026Z"
    },
    "index": 49
   },
   "outputs": [],
   "source": [
    "# np.diag()\n",
    "\n",
    "np.diag(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:27.569049Z",
     "start_time": "2021-05-11T16:48:27.565212Z"
    },
    "index": 51
   },
   "outputs": [],
   "source": [
    "# Reconstruct A by multiplication\n",
    "\n",
    "vectors.dot(np.diag(values)).dot(vectors.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 53
   },
   "source": [
    "## PCA by Hand\n",
    "\n",
    "What follows is indebted to [Sebastian Raschka](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:28.856567Z",
     "start_time": "2021-05-11T16:48:28.852683Z"
    },
    "index": 54
   },
   "outputs": [],
   "source": [
    "# We'll start by producing the covariance matrix for the columns of X_tr_pp.\n",
    "\n",
    "cov_mat = np.cov(X_tr_pp, rowvar=False)\n",
    "cov_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:29.349318Z",
     "start_time": "2021-05-11T16:48:29.344563Z"
    },
    "index": 56
   },
   "outputs": [],
   "source": [
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:30.386978Z",
     "start_time": "2021-05-11T16:48:30.381567Z"
    },
    "index": 58,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:31.734638Z",
     "start_time": "2021-05-11T16:48:31.732023Z"
    },
    "index": 60
   },
   "outputs": [],
   "source": [
    "# Let's assign the results of eig(cov_mat) to a double of variables.\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:32.350930Z",
     "start_time": "2021-05-11T16:48:32.346324Z"
    },
    "index": 62
   },
   "outputs": [],
   "source": [
    "# The columns of \"eigvecs\" are the eigenvectors!\n",
    "\n",
    "eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:33.706028Z",
     "start_time": "2021-05-11T16:48:33.703259Z"
    },
    "index": 64
   },
   "outputs": [],
   "source": [
    "# The eigenvectors of the covariance matrix are our principal components.\n",
    "# Let's look at the first three.\n",
    "\n",
    "pcabh = np.vstack([row[:3] for row in eigvecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:34.316802Z",
     "start_time": "2021-05-11T16:48:34.313097Z"
    },
    "index": 66
   },
   "outputs": [],
   "source": [
    "pcabh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 68
   },
   "source": [
    "Now, to transform our data points into the space defined by the principal components, we simply need to compute the dot-product of `X_tr_pp` with those principal components.\n",
    "\n",
    "Why? Think about what this matrix product looks like:\n",
    "\n",
    "We take a row of `X_tr_pp` and multiply it by a column of `pcabh`, pairwise. The row of `X_tr_pp` represents the values for the columns in the original space. The column of `pcabh` represents the weights we need on each of the original columns in order to transform a value into principal-component space. And so the product of these two matrices will be each row, transformed into principal-component space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:35.830841Z",
     "start_time": "2021-05-11T16:48:35.826833Z"
    },
    "index": 69
   },
   "outputs": [],
   "source": [
    "X_tr_pp[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:36.807409Z",
     "start_time": "2021-05-11T16:48:36.803351Z"
    },
    "index": 71
   },
   "outputs": [],
   "source": [
    "X_tr_pp.dot(pcabh)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 73
   },
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:37.518592Z",
     "start_time": "2021-05-11T16:48:37.514425Z"
    },
    "index": 74
   },
   "outputs": [],
   "source": [
    "# Naturally, sklearn has a shortcut for this!\n",
    "\n",
    "pca = PCA(n_components=3) # Check out how `n_components` works\n",
    "\n",
    "X_train_new = pca.fit_transform(X_tr_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:38.739823Z",
     "start_time": "2021-05-11T16:48:38.736650Z"
    },
    "index": 76
   },
   "outputs": [],
   "source": [
    "# Let's check out the explained variance\n",
    "\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:39.770718Z",
     "start_time": "2021-05-11T16:48:39.767105Z"
    },
    "index": 78
   },
   "outputs": [],
   "source": [
    "# The ratio is often more informative\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:41.526948Z",
     "start_time": "2021-05-11T16:48:41.523283Z"
    },
    "index": 80,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can also check out the Principal Components themselves\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:48:42.864133Z",
     "start_time": "2021-05-11T16:48:42.860618Z"
    },
    "index": 82
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 84
   },
   "source": [
    "The results of our PCA are as follows: \n",
    "\n",
    "**PC1** = 0.450 * cylinders_sd + 0.464 * cubicinches_sd + 0.455 * hp_sd + 0.433 * 𝑙𝑏𝑠_𝑠𝑑 - 0.350 * time-to-60_sd - 0.188 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.141 * US\n",
    "\n",
    "**PC2** = -0.132 * cylinders_sd - 0.1 * cubicinches_sd + 0.005 * hp_sd  -0.194 * 𝑙𝑏𝑠_𝑠𝑑 - 0.123 * time-to-60_sd - 0.938 * year_sd + 0.13 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.189 * cylinders_sd + 0.142 * cubicinches_sd - 0.143 * hp_sd + 0.341 * 𝑙𝑏𝑠_𝑠𝑑 + 0.851 * time-to-60_sd - 0.236 * year_sd - 0.041 * Europe - 0.132 * Japan + 0.091 * US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 85
   },
   "source": [
    "## Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 86
   },
   "source": [
    "These principal components should also be mutually orthogonal. If they are, then the dot product of any two of them should be 0. Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:03.202857Z",
     "start_time": "2021-05-11T16:49:03.199212Z"
    },
    "index": 87
   },
   "outputs": [],
   "source": [
    "pca.components_[0].dot(pca.components_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:03.640877Z",
     "start_time": "2021-05-11T16:49:03.637099Z"
    },
    "index": 89
   },
   "outputs": [],
   "source": [
    "pca.components_[0].dot(pca.components_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:04.147442Z",
     "start_time": "2021-05-11T16:49:04.143872Z"
    },
    "index": 91
   },
   "outputs": [],
   "source": [
    "pca.components_[1].dot(pca.components_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 93
   },
   "source": [
    "## Transformed dimensions have zero correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:06.140889Z",
     "start_time": "2021-05-11T16:49:06.136574Z"
    },
    "index": 94
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X_train_new.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 96
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:07.735045Z",
     "start_time": "2021-05-11T16:49:07.732381Z"
    },
    "index": 97
   },
   "outputs": [],
   "source": [
    "X_test_new = pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:08.258635Z",
     "start_time": "2021-05-11T16:49:08.255864Z"
    },
    "index": 99
   },
   "outputs": [],
   "source": [
    "# Reassembling the whole dataset for the sake of visualization\n",
    "\n",
    "X_transformed = np.vstack([X_train_new, X_test_new])\n",
    "y_new = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:08.786763Z",
     "start_time": "2021-05-11T16:49:08.662472Z"
    },
    "index": 101
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 0], y_new, 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:09.829552Z",
     "start_time": "2021-05-11T16:49:09.661840Z"
    },
    "index": 103
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 1], y_new, 'g.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:10.463020Z",
     "start_time": "2021-05-11T16:49:10.347645Z"
    },
    "index": 105
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 2], y_new, 'k.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:15.677406Z",
     "start_time": "2021-05-11T16:49:15.667878Z"
    },
    "index": 107
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X_transformed, y_new[:, np.newaxis]]),\n",
    "                  columns=['PC1', 'PC2', 'PC3', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:41.812062Z",
     "start_time": "2021-05-11T16:49:41.465677Z"
    },
    "index": 109,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.relplot(data=df,\n",
    "            x='PC1',\n",
    "            y='PC2',\n",
    "           hue='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 111
   },
   "source": [
    "## Relation to Linear Regression\n",
    "\n",
    "Question: Is the first principal component the same line we would get if we constructed an ordinary least-squares regression line?\n",
    "\n",
    "Answer: No. The best-fit line minimizes the sum of squared errors, i.e. the minimum sum of (\"vertical\") distances between the predictions and the real values of the dependent variable. Principal Component Analysis, by contrast, is not a modeling procedure and so has no target. The first principal component thus cannot minimize the sum of distances between predictions and real values; instead, it minimizes the sum of (\"perpendicular\") distances between the data points and *it (the line) itself*.\n",
    "\n",
    "Suppose we look at MPG vs. z-scores of weight in lbs. Let's make a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:44.571278Z",
     "start_time": "2021-05-11T16:49:44.423767Z"
    },
    "index": 112
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "\n",
    "a.scatter(X_tr_pp[:, 1], y_train)\n",
    "a.set_xlabel('weight z-scores (lbs.)')\n",
    "a.set_ylabel('efficiency (MPG)')\n",
    "a.set_title('MPG vs. Weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 114
   },
   "source": [
    "Let's add the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:47.099063Z",
     "start_time": "2021-05-11T16:49:47.094468Z"
    },
    "index": 115
   },
   "outputs": [],
   "source": [
    "beta1 = LinearRegression().fit(X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                               y_train).coef_\n",
    "beta0 = LinearRegression().fit(X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                               y_train).intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:51.924084Z",
     "start_time": "2021-05-11T16:49:51.760638Z"
    },
    "index": 117
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "\n",
    "a.scatter(X_tr_pp[:, 1], y_train)\n",
    "a.plot(X_tr_pp[:, 1],\n",
    "       beta1[0] * X_tr_pp[:, 1] + beta0,\n",
    "      c='r', label='best-fit line')\n",
    "a.set_xlabel('weight z-scores (lbs.)')\n",
    "a.set_ylabel('efficiency (MPG)')\n",
    "a.set_title('MPG vs. Weight')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 119
   },
   "source": [
    "Now let's see what the principal component looks like. We'll make use of the `inverse_transform()` method of `PCA()` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:49:53.022462Z",
     "start_time": "2021-05-11T16:49:53.016392Z"
    },
    "index": 120
   },
   "outputs": [],
   "source": [
    "pc1 = PCA(n_components=1).fit(np.concatenate((X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                                 y_train.values.reshape(-1, 1)),\n",
    "                                axis=1))\n",
    "\n",
    "pc = pc1.transform(np.concatenate((X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                                 y_train.values.reshape(-1, 1)),\n",
    "                                axis=1))\n",
    "\n",
    "pc_inv = pc1.inverse_transform(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:50:16.082046Z",
     "start_time": "2021-05-11T16:50:15.914763Z"
    },
    "index": 122
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "\n",
    "a.scatter(X_tr_pp[:, 1], y_train)\n",
    "a.plot(X_tr_pp[:, 1],\n",
    "       beta1[0] * X_tr_pp[:, 1] + beta0,\n",
    "      c='r', label='best-fit line')\n",
    "a.plot(pc_inv[:, 0],\n",
    "       pc_inv[:, 1],\n",
    "      c='b', label='principal component')\n",
    "a.set_xlabel('weight z-scores (lbs.)')\n",
    "a.set_ylabel('efficiency (MPG)')\n",
    "a.set_title('MPG vs. Weight')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 124
   },
   "source": [
    "Check out this post, to which I am indebted, for more on this subtle point: https://shankarmsy.github.io/posts/pca-vs-lr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 125
   },
   "source": [
    "## Modeling with New Dimensions\n",
    "\n",
    "Now that we have optimized our features, we can build a new model with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:50:18.665840Z",
     "start_time": "2021-05-11T16:50:18.659936Z"
    },
    "index": 126
   },
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_new, y_train)\n",
    "lr_pca.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:50:19.407566Z",
     "start_time": "2021-05-11T16:50:19.404650Z"
    },
    "index": 128
   },
   "outputs": [],
   "source": [
    "X_test_new = pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:50:20.097311Z",
     "start_time": "2021-05-11T16:50:20.093004Z"
    },
    "index": 130
   },
   "outputs": [],
   "source": [
    "lr_pca.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:50:21.288496Z",
     "start_time": "2021-05-11T16:50:21.285061Z"
    },
    "index": 132
   },
   "outputs": [],
   "source": [
    "lr_pca.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 134
   },
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$-2.967\\times PC1 - 1.162\\times PC2 -2.486\\times PC3$\n",
    "\n",
    "Of course, since the principal components are just linear combinations of our original predictors, we could re-express this hyperplane in terms of those original predictors!\n",
    "\n",
    "And if the PCA was worth anything, we should expect the new linear model to be *different from* the first!\n",
    "\n",
    "Recall that we had:\n",
    "\n",
    "**PC1** = 0.450 * cylinders_sd + 0.464 * cubicinches_sd + 0.455 * hp_sd + 0.433 * 𝑙𝑏𝑠_𝑠𝑑 - 0.350 * timeto60sd - 0.188 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.141 * US\n",
    "\n",
    "**PC2** = -0.132 * cylinders_sd - 0.1 * cubicinches_sd + 0.005 * hp_sd  -0.194 * 𝑙𝑏𝑠_𝑠𝑑 - 0.123 * timeto60sd - 0.938 * year_sd + 0.13 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.189 * cylinders_sd + 0.142 * cubicinches_sd - 0.143 * hp_sd + 0.341 * 𝑙𝑏𝑠_𝑠𝑑 + 0.851 * timeto60sd - 0.236 * year_sd - 0.041 * Europe - 0.132 * Japan + 0.091 * US\n",
    "\n",
    "Therefore, our new PCA-made hyperplane can be expressed as:\n",
    "\n",
    "$-2.967\\times(0.450 * cylinderssd + 0.464 * cubicinchessd + 0.455 * hpsd + 0.433 * 𝑙𝑏𝑠𝑠𝑑 - 0.350 * timeto60sd - 0.188 * yearsd - 0.068 * Europe - 0.073 * Japan + 0.141 * US)$ <br/> $- 1.162\\times(-0.132 * cylinderssd - 0.1 * cubicinchessd + 0.005 * hpsd -0.194 * 𝑙𝑏𝑠𝑠𝑑 - 0.123 * timeto60sd - 0.938 * yearsd + 0.13 * Europe + 0.022 * Japan - 0.152 * US)$ <br/> $- 2.486\\times(0.189 * cylinderssd + 0.142 * cubicinchessd - 0.143 * hpsd + 0.341 * 𝑙𝑏𝑠𝑠𝑑 + 0.851 * timeto60sd - 0.236 * yearsd - 0.041 * Europe - 0.132 * Japan + 0.091 * US)$\n",
    "\n",
    "Let's make these calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:50:58.756084Z",
     "start_time": "2021-05-11T16:50:58.751682Z"
    },
    "index": 135
   },
   "outputs": [],
   "source": [
    "def pca_original(feature_names, model, pca, class_index=1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns the coefficients for a model that has been reduced\n",
    "    with sklearn's PCA.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    coeffs = {}\n",
    "    # For multi class classification problems, model.coef_\n",
    "    # returns a matrix of coefficients for each class\n",
    "    # If model.coef_.shape[1] exists and is not 0\n",
    "    # then the coefficients are collected for the desired\n",
    "    # class\n",
    "    try:\n",
    "        if model.coef_.shape[1]:\n",
    "            weights = model.coef_[class_index]\n",
    "        else:\n",
    "            weights = model.coef_\n",
    "    except:\n",
    "        weights = model.coef_\n",
    "    \n",
    "    for idx in range(len(feature_names)):\n",
    "        coeffs[feature_names[idx]] = np.round(weights @ pca.components_[:,idx], 3)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:51:00.046013Z",
     "start_time": "2021-05-11T16:51:00.041509Z"
    },
    "index": 137
   },
   "outputs": [],
   "source": [
    "feature_names = ['cylinders_sd', 'cubicinches_sd', 'horsepower_sd', \n",
    "                 'weightlbs_sd','timeto60_sd', 'year_sd', 'Europe',\n",
    "                'Japan', 'US']\n",
    "\n",
    "pca_original(feature_names, lr_pca, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 139
   },
   "source": [
    "So our best-fit hyperplane using PCA is:\n",
    "\n",
    "$-1.659\\times cyl\\_sd -1.62\\times in^3\\_sd-1.003\\times hp\\_sd-1.911\\times lbs.\\_sd -0.936\\times time_{60}\\_sd + 2.237\\times yr\\_sd -0.052\\times brand_{Europe} + 0.52\\times brand_{Japan} -0.468\\times brand_{US}$\n",
    "\n",
    "\n",
    "Recall that our first linear regression model had:\n",
    "\n",
    "$- 1.555\\times cyl\\_sd + 2.189\\times in^3\\_sd - 1.154\\times hp\\_sd - 4.681\\times lbs.\\_sd  - 0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$\n",
    "\n",
    "which is clearly a different hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T01:46:43.165835Z",
     "start_time": "2021-05-11T01:46:43.163819Z"
    },
    "index": 140
   },
   "source": [
    "# Importance of scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:27.583914Z",
     "start_time": "2021-05-11T16:52:27.548178Z"
    },
    "index": 141
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:28.238685Z",
     "start_time": "2021-05-11T16:52:28.231528Z"
    },
    "index": 143
   },
   "outputs": [],
   "source": [
    "unscaled = make_pipeline(PCA(.95), LogisticRegression())\n",
    "scaled = make_pipeline(StandardScaler(), PCA(.95), LogisticRegression())\n",
    "data = load_wine()\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:28.978533Z",
     "start_time": "2021-05-11T16:52:28.889110Z"
    },
    "index": 145
   },
   "outputs": [],
   "source": [
    "cross_val_score(unscaled, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:29.454155Z",
     "start_time": "2021-05-11T16:52:29.378808Z"
    },
    "index": 147
   },
   "outputs": [],
   "source": [
    "cross_val_score(scaled, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:30.628580Z",
     "start_time": "2021-05-11T16:52:30.361597Z"
    },
    "index": 149
   },
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca = pd.DataFrame(X_pca, columns=['f1', 'f2'])\n",
    "X_scaled_pca = pca.fit_transform(X_scaled)\n",
    "X_scaled_pca = pd.DataFrame(X_scaled_pca, columns=['f1', 'f2'])\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
    "for label in pd.Series(y).unique():\n",
    "    frame = X_pca[y==label]\n",
    "    frame_scaled = X_scaled_pca[y==label]\n",
    "    ax[0].scatter(frame.f1, frame.f2, label=label)\n",
    "    ax[1].scatter(frame_scaled.f1, frame_scaled.f2, label=label)\n",
    "ax[0].set_title('PCA Unscaled')\n",
    "ax[1].set_title('PCA Scaled')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T01:59:58.097235Z",
     "start_time": "2021-05-11T01:59:58.095136Z"
    },
    "index": 151
   },
   "source": [
    "# Selecting `n_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:33.766158Z",
     "start_time": "2021-05-11T16:52:33.653807Z"
    },
    "index": 152
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_scaled)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Percentage of explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:34.809411Z",
     "start_time": "2021-05-11T16:52:34.736365Z"
    },
    "index": 154
   },
   "outputs": [],
   "source": [
    "pca = PCA(12)\n",
    "pipeline = make_pipeline(StandardScaler(), pca, LogisticRegression())\n",
    "cross_val_score(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:35.555589Z",
     "start_time": "2021-05-11T16:52:35.535970Z"
    },
    "index": 156
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:52:36.414578Z",
     "start_time": "2021-05-11T16:52:36.409546Z"
    },
    "index": 158
   },
   "outputs": [],
   "source": [
    "pca_original(X.columns, pipeline.steps[-1][1], pipeline.steps[-2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 160
   },
   "source": [
    "## Extra Resource\n",
    "\n",
    "- [StatQuests Longform PCA video](https://www.youtube.com/watch?v=_UVHneBUBW0)\n",
    "- [Three Blue One Brown Video on Eigan Vectors](https://www.youtube.com/watch?v=PFDu9oVAE-g)\n",
    "- [Python Data Science Handbook - In Depth PCA](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html#:~:text=PCA%20is%20fundamentally%20a%20dimensionality,and%20engineering%2C%20and%20much%20more)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
