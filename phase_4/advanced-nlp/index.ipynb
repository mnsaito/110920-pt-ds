{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Topic 39+: Deeper NLP\n",
    "\n",
    "1. Word vectors\n",
    "    - Word vectors with Gensim\n",
    "    - Word vectors with SpaCy\n",
    "2. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 2
   },
   "source": [
    "# Word Embedding\n",
    "\n",
    "Embedding vectors are very different than the TF-IDF or Count Vectors we learned about previously.  TF-IDF and Count Vectors contain only information about the quantity of each word in a document, but nothing about the words' meanings.  \n",
    "\n",
    "Embedding vectors capture the semantic meaning of words.  Think about that for second.  How can you turn the MEANING of a work into a vector of numbers?\n",
    "\n",
    "Count the TF-IDF transforms a document into a sparse vector similar to a one-hot encoding (but with values not limited to 0 and 1).  Embedding vectors transform each word (there are ways to transform a sentence or document into one vector, but we'll talk about that later) into a vector in an arbitrarily high dimensional vector space.  In this case the vector is not sparse, but describes the position of the word in each dimension in that space.  This just like how (.5, .3, .7) would describe the position of a point in 3 dimensional space (x, y, z). Word vector spaces can have 50, 100, 500, or more dimensions.\n",
    "\n",
    "**What does a dimension represent?** \n",
    "\n",
    "A dimension in this space represents a relationship between words.  For instance, dimension x may represent gender, and dimension y may represent social status.  \n",
    "\n",
    "**How are these vectors determined?** \n",
    "\n",
    "Embeddings are learned by an unsupervised model, somewhat like PCA.  The model trains on a corpus to determine how words are related to each other in the texts.\n",
    "\n",
    "The embeddings can be learned from your corpus of documents through models like Word2Vec or can be downloaded from pretrained embedding models.\n",
    "\n",
    "\n",
    "**Bias Alert**\n",
    "\n",
    "The dimensions in an embedding model can and do represent bias inherent in language.  Dimensions can represent semantic relationships such as race, gender, ability, sexuality, etc.  'Doctor' and 'Custodian' may occupy different positions along a racial or gender dimension!  There are ways to reduce this bias by collapsing a dimension and projecting it onto a lower dimensional space.  A math heavy and comprehensive paper released by Stanford researchers is available [Here](http://cs229.stanford.edu/proj2016/report/BadieChakrabortyRudder-ReducingGenderBiasInWordEmbeddings-report.pdf)\n",
    "\n",
    "If you want to test bias in your embedding model, try an analogy like: \"'man' is to 'doctor' as woman is to 'X'\". You'll learn below how to ask your model to complete these analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 3
   },
   "source": [
    "### Gensim Documentation\n",
    "\n",
    "* Pretrained vectors: https://github.com/RaRe-Technologies/gensim-data\n",
    "* Vector methods: https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 4
   },
   "source": [
    "**GloVe Vectors**\n",
    "\n",
    "GloVe vectors are a set of word embedding models pre-trained at Stanford and available for free.  There is a collection of different models available.  The one we use below projects words into a 100 dimensional space and is trained on the full corpus of Wikipedia plus a the Gigaword 5 collection gathered from various news sources.  Documentation can be found [here](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 5
   },
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 6
   },
   "source": [
    "## Vector Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 7
   },
   "outputs": [],
   "source": [
    "word_vectors['coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 8
   },
   "source": [
    "## Word similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 9
   },
   "outputs": [],
   "source": [
    "word_vectors.most_similar('coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 10
   },
   "outputs": [],
   "source": [
    "word_vectors.most_similar('hilton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 11
   },
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 12
   },
   "source": [
    "## Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 13
   },
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = word_vectors.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "analogy('japan', 'japanese', 'australia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 15
   },
   "outputs": [],
   "source": [
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 16
   },
   "outputs": [],
   "source": [
    "analogy('obama', 'clinton', 'reagan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 17
   },
   "outputs": [],
   "source": [
    "analogy('tall', 'tallest', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 18
   },
   "outputs": [],
   "source": [
    "analogy('particular', 'fussy', 'subservient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Bias\n",
    "\n",
    "What are your thought about the below results?  Does this word embedding model contain bias?  Feel free to try some more to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy('white','doctor','black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 19
   },
   "source": [
    "## Odd One Out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 20
   },
   "outputs": [],
   "source": [
    "word_vectors.doesnt_match(\"england france germany russia\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 21
   },
   "source": [
    "## Embedding Sentences/Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 22
   },
   "outputs": [],
   "source": [
    "sentence = 'I like my coffee hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 23
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for w in sentence.split():\n",
    "    try:\n",
    "        vectors.append(word_vectors[w])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 24
   },
   "outputs": [],
   "source": [
    "#Sum the vectors to create an embedding vector that represents the entire sentence.\n",
    "sum(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "## Graphical Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 26
   },
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 27
   },
   "outputs": [],
   "source": [
    "display_pca_scatterplot(word_vectors, \n",
    "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
    "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "                         'school', 'college', 'university', 'institute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 28
   },
   "source": [
    "## SpaCy\n",
    "\n",
    "SpaCy is a very powerful NLP library that can be used for many of the functions that the NLTK package provides (NLTK is still often used for its list of stopwords), plus word embedding models, and MORE!  If you are interested in NLP, I SERIOUSLY recommend you check out what SpaCy can do.\n",
    "\n",
    "* Available SpaCy libraries: https://spacy.io/usage/models\n",
    "* Documentation: https://spacy.io/usage/processing-pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 29
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 30
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"../resources/nlp_classification.csv\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 31
   },
   "outputs": [],
   "source": [
    "raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 32
   },
   "source": [
    "### SpaCy Objects\n",
    "\n",
    "The first step in unlocking the power of SpaCy is to convert your documents into SpaCy objects.  This is done by downloading a model, such as en_core_web_sm (english, core, trained on the web, small version) and using it to predict on each of your documents, which transforms them into SpaCy objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 33
   },
   "outputs": [],
   "source": [
    "### This installs spacy if you need\n",
    "# !pip install spacy\n",
    "\n",
    "### This downloads the specific pretrained word embeddings\n",
    "\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 34
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# df.progress_apply() applies a function to your dataframe and shows a progress bar\n",
    "\n",
    "raw['spacy'] = raw.body.progress_apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 36
   },
   "source": [
    "## SpaCy Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 37
   },
   "outputs": [],
   "source": [
    "# now each element under \"spacy\" is its own object!\n",
    "first_spacy = raw.spacy[0]\n",
    "print(type(first_spacy))\n",
    "print(type(first_spacy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 38
   },
   "source": [
    "* https://spacy.io/api/token\n",
    "* https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 39
   },
   "outputs": [],
   "source": [
    "print(len(first_spacy.vector))\n",
    "first_spacy.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 40
   },
   "outputs": [],
   "source": [
    "print(len(first_spacy[0].vector))\n",
    "first_spacy[0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 41
   },
   "source": [
    "## Spacy Parts of Speech (pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "[w.pos_ for w in first_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 43
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.vstack([x.vector for x in raw.spacy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 44
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 45
   },
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Topic Modeling is an unsupervised modeling technique that extracts common keywords from corpora to determine which topics are commonly discussed.  This can be useful for determining classes to assign to texts algorithmically when topics are not known.  It's also useful to data exploration to better understand your corpus.\n",
    "\n",
    "If you want a more comprehensive guide and explanation of topic modeling, I'll refer you to [this article](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) by Selva Prabhakaran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 46
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 47
   },
   "source": [
    "### Functionalize\n",
    "\n",
    "It's always a good idea to functionalize your text processing pipeline so you can reuse it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 48
   },
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stopwords.words(\"english\"), allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    #use embedded list constructors to iterate over each word in the corpus\n",
    "    texts = [[word for word in doc.split() if word not in stop_words] for doc in texts]\n",
    "    texts_out = []\n",
    "    \n",
    "    #load your SpaCy model\n",
    "    nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        #SpaCy allows you to use the parts of speech of each word to guide lemmatization.\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in doc if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(raw.body) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 49
   },
   "outputs": [],
   "source": [
    "data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 50
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 51
   },
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208.452px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
