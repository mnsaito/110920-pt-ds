{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.100689Z",
     "start_time": "2021-02-11T17:58:12.913838Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import linear_rainbow, het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Modeling Workflow\n",
    "\n",
    "This dataset was downloaded from [Kaggle](https://www.kaggle.com/kumarajarshi/life-expectancy-who) and reflects data collected by the WHO about life expectancy and potentially-related factors.  The information is aggregated on a per-country per-year basis.\n",
    "\n",
    "The following questions have been posed:\n",
    "\n",
    "1. Does various predicting factors which has been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy?\n",
    "2. Should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?\n",
    "3. How does Infant and Adult mortality rates affect life expectancy?\n",
    "4. Does Life Expectancy has positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc.\n",
    "5. What is the impact of schooling on the lifespan of humans?\n",
    "6. Does Life Expectancy have positive or negative relationship with drinking alcohol?\n",
    "7. Do densely populated countries tend to have lower life expectancy?\n",
    "8. What is the impact of Immunization coverage on life Expectancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.122714Z",
     "start_time": "2021-02-11T17:58:15.102527Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"life_expectancy.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.203363Z",
     "start_time": "2021-02-11T17:58:15.170315Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.270136Z",
     "start_time": "2021-02-11T17:58:15.205247Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.275336Z",
     "start_time": "2021-02-11T17:58:15.271681Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Preparation\n",
    "\n",
    "The original column names have extra spaces and other irregularities.  Let's clean those up, and also move the target variable to be the first column, for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.282477Z",
     "start_time": "2021-02-11T17:58:15.276620Z"
    }
   },
   "outputs": [],
   "source": [
    "# rename so everything is snake_case\n",
    "df = df.rename(columns={\n",
    "    'Life expectancy ': 'Life_Expectancy',\n",
    "    'Adult Mortality': 'Adult_Mortality',\n",
    "    'infant deaths': 'Infant_Deaths',\n",
    "    'percentage expenditure': 'Percentage_Expenditure',\n",
    "    'Hepatitis B': 'Hepatitis_B',\n",
    "    'Measles ': 'Measles',\n",
    "    ' BMI ': 'BMI',\n",
    "    'under-five deaths ': 'Under_five_Deaths',\n",
    "    'Total expenditure': 'Total_Expenditure',\n",
    "    'Diphtheria ': 'Diptheria',\n",
    "    ' HIV/AIDS': 'HIV_AIDS',\n",
    "    ' thinness  1-19 years': 'Thinness_1_19_years',\n",
    "    ' thinness 5-9 years': 'Thinness_5_9_years',\n",
    "    'Income composition of resources': 'Income_Composition_of_Resources'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.306503Z",
     "start_time": "2021-02-11T17:58:15.283810Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.312101Z",
     "start_time": "2021-02-11T17:58:15.308703Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.317626Z",
     "start_time": "2021-02-11T17:58:15.313536Z"
    }
   },
   "outputs": [],
   "source": [
    "# reorder so life expectancy is the first column\n",
    "cols = list(df.columns)\n",
    "cols = [cols[3]] + cols[:3] + cols[4:]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.342305Z",
     "start_time": "2021-02-11T17:58:15.318956Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "There are a lot of variables here!  Let's look at a correlation matrix to see which ones might be the most useful.  (Here we are looking for variables that are highly correlated with the target variable, but not highly correlated with other input variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:15.899382Z",
     "start_time": "2021-02-11T17:58:15.343774Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "# The mask is not necessary, but corr() has duplicate values on either side of the diagonal\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr, mask=mask, ax=ax1, cmap=\"viridis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it looks like there are only a few that are highly positively correlated with life expectancy.  Let's make a pair plot of those.\n",
    "\n",
    "(Note: we don't want to do a pair plot right from the outset because it would be too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.053727Z",
     "start_time": "2021-02-11T17:58:15.900495Z"
    }
   },
   "outputs": [],
   "source": [
    "positively_correlated_cols = ['Life_Expectancy','BMI', 'Income_Composition_of_Resources', 'Schooling']\n",
    "positively_correlated_df = df[positively_correlated_cols]\n",
    "sns.pairplot(positively_correlated_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it looks like the correlation with BMI is a little fuzzier than the others, so let's exclude it for now.  `Schooling` and `Income_Composition_of_Resources` are highly correlated with both life expectancy and each other, so let's only include one of them.  `Schooling` seems like a good choice because it would allow us to answer Question 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.060529Z",
     "start_time": "2021-02-11T17:58:19.054763Z"
    }
   },
   "outputs": [],
   "source": [
    "fsm_df = df[[\"Schooling\", \"Life_Expectancy\"]].copy()\n",
    "fsm_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.095839Z",
     "start_time": "2021-02-11T17:58:19.061920Z"
    }
   },
   "outputs": [],
   "source": [
    "fsm = ols(formula=\"Life_Expectancy ~ Schooling\", data=fsm_df)\n",
    "fsm_results = fsm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.112303Z",
     "start_time": "2021-02-11T17:58:19.097381Z"
    }
   },
   "outputs": [],
   "source": [
    "fsm_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad.  We are only explaining about 57% of the variance in life expectancy, but we only have one feature so far and it's statistically significant at an alpha of 0.05.\n",
    "\n",
    "We could stop right now and say that according to our model:\n",
    "\n",
    " - A country with zero years of schooling on average is expected to have a life expectancy of 44.1 years\n",
    " - For each additional average year of schooling, we expect life expectancy to increase by 2.1 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we move forward, let's also check for the assumptions of linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity\n",
    "\n",
    "Linear regression assumes that the input variable linearly predicts the output variable.  We already qualitatively checked that with a scatter plot.  But I also think it's a good idea to use a statistical test.  This one is the [Rainbow test](https://www.tandfonline.com/doi/abs/10.1080/03610928208828423) which is available from the [diagnostic submodule of StatsModels](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.linear_rainbow.html#statsmodels.stats.diagnostic.linear_rainbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.120741Z",
     "start_time": "2021-02-11T17:58:19.113936Z"
    }
   },
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(fsm_results)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\", rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the model is linearly predicted by the features, alternative hypothesis is that it is not.  Thus returning a low p-value means that the current model violates the linearity assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality\n",
    "\n",
    "Linear regression assumes that the residuals are normally distributed.  It is possible to check this qualitatively with a Q-Q plot, but this example shows how to assess it statistically.\n",
    "\n",
    "The [Jarque-Bera](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test) test is performed automatically as part of the model summary output, labeled **Jarque-Bera (JB)** and **Prob(JB)**.\n",
    "\n",
    "The null hypothesis is that the residuals are normally distributed, alternative hypothesis is that they are not.  Thus returning a low p-value means that the current model violates the normality assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homoscadasticity\n",
    "\n",
    "Linear regression assumes that the variance of the dependent variable is homogeneous across different value of the independent variable(s).  We can visualize this by looking at the predicted life expectancy vs. the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.125098Z",
     "start_time": "2021-02-11T17:58:19.122411Z"
    }
   },
   "outputs": [],
   "source": [
    "y = fsm_df[\"Life_Expectancy\"]\n",
    "y_hat = fsm_results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.332110Z",
     "start_time": "2021-02-11T17:58:19.126598Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set(xlabel=\"Predicted Life Expectancy\",\n",
    "        ylabel=\"Residuals (Actual - Predicted Life Expectancy)\")\n",
    "ax2.scatter(x=y_hat, y=y-y_hat, color=\"blue\", alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just visually inspecting this, it seems like our model over-predicts life expectancy between 60 and 70 years old in a way that doesn't happen for other age groups.  Plus we have some weird-looking data in the lower end that we might want to inspect.  Maybe there was something wrong with recording those values, or maybe there is something we can feature engineer once we have more independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also run a statistical test.  The [Breusch-Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) is available from the [diagnostic submodule of StatsModels](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_breuschpagan.html#statsmodels.stats.diagnostic.het_breuschpagan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:19.339259Z",
     "start_time": "2021-02-11T17:58:19.333284Z"
    }
   },
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, fsm_df[[\"Schooling\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is homoscedasticity, alternative hypothesis is heteroscedasticity.  Thus returning a low p-value means that the current model violates the homoscedasticity assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence\n",
    "\n",
    "The independence assumption means that the independent variables must not be too collinear.  Right now we have only one independent variable, so we don't need to check this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features to the Model\n",
    "\n",
    "So far, all we have is a simple linear regression.  Let's start adding features to make it a multiple regression.\n",
    "\n",
    "First, I'll repeat the process of the highly positively correlated variables, but this time with the highly negatively correlated variables (based on looking at the correlation matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.660962Z",
     "start_time": "2021-02-11T17:58:19.340645Z"
    }
   },
   "outputs": [],
   "source": [
    "negatively_correlated_cols = [\n",
    "    'Life_Expectancy',\n",
    "    'Adult_Mortality',\n",
    "    'HIV_AIDS',\n",
    "    'Thinness_1_19_years',\n",
    "    'Thinness_5_9_years'\n",
    "]\n",
    "negatively_correlated_df = df[negatively_correlated_cols]\n",
    "sns.pairplot(negatively_correlated_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Adult_Mortality` seems most like a linear relationship to me.  Also, the two thinness metrics seem to be providing very similar information, so we almost certainly should not include both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick experiment to try to flatten out that curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.930145Z",
     "start_time": "2021-02-11T17:58:24.662476Z"
    }
   },
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "ax3.set(xlabel=\"Adult_Mortality\", ylabel=\"Life_Expectancy\")\n",
    "ax3.scatter(x=np.sqrt(df[\"Adult_Mortality\"]), y=df[\"Life_Expectancy\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives me straighter lines, but seems to indicate that we probably need at least two separate models to represent this data correctly.  However in the interest of time, I'm just going to assume the relationship is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.937687Z",
     "start_time": "2021-02-11T17:58:24.931753Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2_df = df[[\"Life_Expectancy\", \"Schooling\", \"Adult_Mortality\"]].copy()\n",
    "model_2_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.962903Z",
     "start_time": "2021-02-11T17:58:24.941667Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2 = ols(formula=\"Life_Expectancy ~ Schooling + Adult_Mortality\", data=model_2_df)\n",
    "model_2_results = model_2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.984524Z",
     "start_time": "2021-02-11T17:58:24.965376Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model Evaluation\n",
    "\n",
    "Adding another feature improved the r-squared from 0.565 to 0.714\n",
    "\n",
    "But let's also look at the model assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.992409Z",
     "start_time": "2021-02-11T17:58:24.986165Z"
    }
   },
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(model_2_results)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\", rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an alpha of 0.05, we are no longer violating the linearity assumption (just barely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality\n",
    "\n",
    "The **Jarque-Bera (JB)** output has gotten worse.  We are still violating the normality assumption.\n",
    "\n",
    "#### Homoscadasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:24.997881Z",
     "start_time": "2021-02-11T17:58:24.994468Z"
    }
   },
   "outputs": [],
   "source": [
    "y = model_2_df[\"Life_Expectancy\"]\n",
    "y_hat = model_2_results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.260627Z",
     "start_time": "2021-02-11T17:58:25.000128Z"
    }
   },
   "outputs": [],
   "source": [
    "fig4, ax4 = plt.subplots()\n",
    "ax4.set(xlabel=\"Predicted Life Expectancy\",\n",
    "        ylabel=\"Residuals (Actual - Predicted Life Expectancy)\")\n",
    "ax4.scatter(x=y_hat, y=y-y_hat, color=\"blue\", alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.271736Z",
     "start_time": "2021-02-11T17:58:25.262851Z"
    }
   },
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_2_df[[\"Schooling\", \"Adult_Mortality\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both visually and numerically, we can see some improvement.  But we are still violating this assumption to a statistically significant degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence\n",
    "\n",
    "You might have noticed in the regression output that there was a warning about the condition number being high.  The condition number is a measure of stability of the matrix used for computing the regression (we'll discuss this more in the next module), and a number above 30 can indicate strong multicollinearity.  Our output is way higher than that.\n",
    "\n",
    "A different (more generous) measure of multicollinearity is the [variance inflation factor](https://en.wikipedia.org/wiki/Variance_inflation_factor).  It is available from the [outlier influence submodule of StatsModels](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html#statsmodels.stats.outliers_influence.variance_inflation_factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.293183Z",
     "start_time": "2021-02-11T17:58:25.273716Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = model_2_df[[\"Schooling\", \"Adult_Mortality\"]].values\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(rows, i) for i in range(2)]\n",
    "vif_df[\"feature\"] = [\"Schooling\", \"Adult_Mortality\"]\n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"rule of thumb\" for VIF is that 5 is too high, so I think it's reasonable to say that we are not violating the independence assumption, despite the high condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Categorical Value\n",
    "\n",
    "This is less realistic than the previous steps but I wanted to give an example\n",
    "\n",
    "In this dataset, we have a lot of numeric values (everything in that correlation matrix), but there are a few that aren't.  One example is `Status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.304243Z",
     "start_time": "2021-02-11T17:58:25.295648Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3_df = df[[\"Life_Expectancy\", \"Schooling\", \"Adult_Mortality\", \"Status\"]].copy()\n",
    "model_3_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.314149Z",
     "start_time": "2021-02-11T17:58:25.306415Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3_df[\"Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.584006Z",
     "start_time": "2021-02-11T17:58:25.315528Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Status\", y=\"Life_Expectancy\", data=model_3_df, kind=\"box\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a difference between the two groups that might be useful to include\n",
    "\n",
    "There are only two categories, so we only need a `LabelEncoder` that will convert the labels into 1s and 0s.  If there were more than two categories, we would use a `OneHotEncoder`, which would create multiple columns out of a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.589982Z",
     "start_time": "2021-02-11T17:58:25.585385Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "status_labels = label_encoder.fit_transform(model_3_df[\"Status\"])\n",
    "status_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.595066Z",
     "start_time": "2021-02-11T17:58:25.591475Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling us that \"Developed\" is encoded as 0 and \"Developing\" is encoded as 1.  This means that \"Developed\" is assumed at the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.601975Z",
     "start_time": "2021-02-11T17:58:25.596702Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3_df[\"Status_Encoded\"] = status_labels\n",
    "model_3_df.drop(\"Status\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.620708Z",
     "start_time": "2021-02-11T17:58:25.603414Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3 = ols(formula=\"Life_Expectancy ~ Schooling + Adult_Mortality + Status_Encoded\", data=model_3_df)\n",
    "model_3_results = model_3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.642711Z",
     "start_time": "2021-02-11T17:58:25.622873Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model Evaluation\n",
    "\n",
    "Adding another feature improved the r-squared a tiny bit from 0.714 to 0.718\n",
    "\n",
    "Let's look at the model assumptions again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.650537Z",
     "start_time": "2021-02-11T17:58:25.644578Z"
    }
   },
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(model_3_results)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\", rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another small improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality\n",
    "\n",
    "The **Jarque-Bera (JB)** output has gotten slightly better.  But we are still violating the normality assumption.\n",
    "\n",
    "#### Homoscadasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.655825Z",
     "start_time": "2021-02-11T17:58:25.652594Z"
    }
   },
   "outputs": [],
   "source": [
    "y = model_3_df[\"Life_Expectancy\"]\n",
    "y_hat = model_3_results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.919962Z",
     "start_time": "2021-02-11T17:58:25.657829Z"
    }
   },
   "outputs": [],
   "source": [
    "fig5, ax5 = plt.subplots()\n",
    "ax5.set(xlabel=\"Predicted Life Expectancy\",\n",
    "        ylabel=\"Residuals (Actual - Predicted Life Expectancy)\")\n",
    "ax5.scatter(x=y_hat, y=y-y_hat, color=\"blue\", alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.929181Z",
     "start_time": "2021-02-11T17:58:25.921656Z"
    }
   },
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_3_df[[\"Schooling\", \"Adult_Mortality\", \"Status_Encoded\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric got worse, although the plot looks fairly similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:58:25.945783Z",
     "start_time": "2021-02-11T17:58:25.930926Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = model_3_df[[\"Schooling\", \"Adult_Mortality\", \"Status_Encoded\"]].values\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(rows, i) for i in range(3)]\n",
    "vif_df[\"feature\"] = [\"Schooling\", \"Adult_Mortality\", \"Status_Encoded\"]\n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF metrics are getting higher, which means that there is stronger multicollinearity.  But we have still not exceeded the threshold of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We started with a baseline model where the only input feature was `Schooling`.  Our baseline model had an r-squared of 0.565.  This model violated the linearity (p < 0.001), normality (p < 0.001), and homoscadasticity (p < 0.001) assumptions of linear regression.  The independence assumption was met by default because there was only one input feature.\n",
    "\n",
    "The final model for this lesson had three input features: `Schooling`, `Adult_Mortality`, and `Status_Encoded`.  It had an r-squared of 0.718.  This model did not violate the linearity assumption (p = 0.084), but it did violate the normality (p < 0.001) and homoscedasticity (p < 0.001) assumptions.  Based on the variance inflaction factor metric, it did not violate the independence assumption.\n",
    "\n",
    "We are able to address the following questions from above:\n",
    "\n",
    "*1. Does various predicting factors which has been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy?*\n",
    "\n",
    "With only 3 features we are able to explain about 71% of the variance in life expectancy.  This indicates that these factors truly are explanatory.  More analysis is required to understand how much additional explanatory power would be provided by incorporating additional features into the model.\n",
    "\n",
    "*3. How does Infant and Adult mortality rates affect life expectancy?*\n",
    "\n",
    "So far we have only investigated adult mortality.  The adult mortality rate (\"probability of dying between 15 and 60 years per 1000 population\") has a negative correlation with life expectancy.  For each increase of 1 in the adult mortality rate, life expectancy decreases by about .03 years.\n",
    "\n",
    "*5. What is the impact of schooling on the lifespan of humans?*\n",
    "\n",
    "In our latest model, we find that each additional year of average schooling is associated with 1.4 years of added life expectancy.  However it is challenging to interpret whether it is schooling that is actually having the impact.  Schooling is highly correlated with `Income_Composition_of_Resources` (\"Human Development Index in terms of income composition of resources\") so it is very possible that schooling is the result of some underlying factor that also impacts life expectancy, rather than schooling impacting life expectancy directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "Things I have not done in this lesson, but that you should consider in your project:\n",
    "\n",
    " - More robust cleaning (possible imputation of missing values, principled exclusion of some data)\n",
    " - Feature scaling\n",
    " - Nearest-neighbors approach (requires more complex feature engineering)\n",
    " - Pulling information from external resources\n",
    " - Removing independent variables if you determine that they are causing too high of multicollinearity\n",
    " - Setting up functions so the code is not so repetitive\n",
    " \n",
    "Also, I've included a dataset called `cars.csv` if you are interested in additional practice that does not use the King County Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
