{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:24:49.340083Z",
     "start_time": "2021-02-04T20:24:49.337502Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of _correlation_ is the simple idea that variables often change _together_. For a simple example, cities with more buses tend to have higher populations.\n",
    "\n",
    "We might observe that, as one variable X increases, so does another Y, OR that as X increases, Y decreases.\n",
    "\n",
    "The _covariance_ describes how two variables co-vary. Note the similarity in the definition to the definition of ordinary variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance\n",
    "\n",
    "For two random variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:19.272636Z",
     "start_time": "2021-02-04T20:17:19.266176Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [1, 3, 5]\n",
    "Y = [2, 9, 10]\n",
    "\n",
    "# Covariance by hand:\n",
    "((1-3) * (2-7) + (3-3) * (9-7) + (5-3) * (10-7)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:20.201205Z",
     "start_time": "2021-02-04T20:17:20.192977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Better yet: With NumPy:\n",
    "np.cov(X, Y, ddof=0)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:21.297651Z",
     "start_time": "2021-02-04T20:17:21.292858Z"
    }
   },
   "outputs": [],
   "source": [
    "np.cov(X, Y, ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:22.728633Z",
     "start_time": "2021-02-04T20:17:22.724898Z"
    }
   },
   "outputs": [],
   "source": [
    "np.var(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is wanted is a _standardized_ scale for covariance, hence: _correlation_.\n",
    "\n",
    "## Correlation\n",
    "\n",
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the $n$'s cancel!).\n",
    "\n",
    "$\\bf{Check}$:\n",
    "\n",
    "<details><summary>\n",
    "What happens if X = Y?\n",
    "</summary>\n",
    "Then numerator = denominator and the correlation = 1!\n",
    "</details>\n",
    "<br/>\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "NumPy also has a correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:25.641405Z",
     "start_time": "2021-02-04T20:17:25.637023Z"
    }
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:26.908518Z",
     "start_time": "2021-02-04T20:17:26.904556Z"
    }
   },
   "outputs": [],
   "source": [
    "4 / np.sqrt(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:27.543407Z",
     "start_time": "2021-02-04T20:17:27.537651Z"
    }
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)[0, 1] == (np.cov(X, Y, ddof=0) / (np.std(X) * np.std(Y)))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so does SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:17:29.027038Z",
     "start_time": "2021-02-04T20:17:29.022431Z"
    }
   },
   "outputs": [],
   "source": [
    "stats.pearsonr(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation\n",
    "\n",
    "_Why_ does it happen that variables correlate? It _may_ be that one is the cause of the other. A city having a high population, for example, probably does have some causal effect on the number of buses that the city has. But this _need not_ be the case, and that is why statisticians are fond of saying that 'correlation is not causation'. An alternative possibility, for example, is that high values of X and Y are _both_ caused by high values of some third factor Z. The size of children's feet, for example, is correlated with their ability to spell, but this is of course NOT because either is a cause of the other. Rather, BOTH are caused by the natural maturing and development of children. As they get older, both their feet and their spelling abilities grow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory\n",
    "\n",
    "It's important at this point to understand the distinction between dependent and independent variables.\n",
    "\n",
    "Roughly, the independent variable is what can be directly manipulated and the dependent variable is what cannot be (but is nevertheless of great interest). What matters structurally is simply that we understand the dependent variable to be a _function_ of the independent variable(s).\n",
    "\n",
    "This is the proper interpretation of a statistical _model_.\n",
    "\n",
    "Simple idea: We can model correlation with a _line_. As one variable changes, so does the other.\n",
    "\n",
    "This model has two *parameters*: *slope* and *y-intercept*.\n",
    "\n",
    "Unless there's a perfectly (and suspiciously) linear relationship between our predictor(s) and our target, there will  be some sort of **error** or **loss** or **residual**. The best-fit line is constructed by minimizing the sum of the squares of these losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression:\n",
    "\n",
    "### 1. The relationship between target and predictor(s) is linear. (Of course!)\n",
    "\n",
    "**How can I check for this?**\n",
    "- Build a scatterplot of y vs. various predictors.\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider log-scaling your data.\n",
    "- Consider a different type of model!\n",
    "\n",
    "### 2. The errors are mutually independent. (That is, there is no correlation between any two errors.)\n",
    "\n",
    "**How can I check for this?**\n",
    "- Build an error plot, i.e. a plot of errors for a particular predictor (vs. the values of that predictor).\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider dropping extreme values.\n",
    "\n",
    "### 3. The errors are normally distributed. (That is, smaller errors are more probable than larger errors, according to the familiar bell curve.)\n",
    "\n",
    "**How can I check for this?**\n",
    "- Check the Omnibus value (More on this later!)\n",
    "- Check the Jarque-Bera value (More on this later!)\n",
    "- Build a QQ-Plot.\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider log-scaling your data.\n",
    "\n",
    "### 4. The errors are homoskedastic. (That is, the errors have the same variance.)\n",
    "\n",
    "**How can I check for this?**\n",
    "- Check the Durbin-Watson score (More on this later!)\n",
    "- Conduct a Goldfeld-Quandt test.\n",
    "- Build an error plot, i.e. a plot of errors for a particular predictor (vs. the values of that predictor).\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider dropping extreme values.\n",
    "- Consider log-scaling your target.\n",
    "- Consider a different type of model!\n",
    "\n",
    "There is no general requirement that the predictors and the target *themselves* be normally distributed. However: Linear regression can work better if the predictors and target are normally distributed. Log-scaling can be a good tool to make data more normal.\n",
    "\n",
    "Suppose e.g. a kde plot of one of my predictors $X_1$ looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![original](images/skewplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case the kde plot of $log(X_1)$ looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log](images/logplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression.\n",
    "\n",
    "Experiment: [Playing with regression line](https://www.desmos.com/calculator/jwquvmikhr) <br/>\n",
    "Limitations: [Anscombe's Quartet](https://www.desmos.com/calculator/paknt6oneh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No Intercept:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T17:46:22.146145Z",
     "start_time": "2021-02-04T17:46:22.139081Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "df = pd.DataFrame([X,Y]).T\n",
    "df.columns = ['X', 'Y']\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(df[['X']], df.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T17:45:35.609348Z",
     "start_time": "2021-02-04T17:45:35.604077Z"
    }
   },
   "source": [
    "**Returning the coefficient:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T17:46:23.752361Z",
     "start_time": "2021-02-04T17:46:23.748814Z"
    }
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Intercept**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T17:47:20.075473Z",
     "start_time": "2021-02-04T17:47:20.069342Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(df[['X']], df.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T17:47:50.256224Z",
     "start_time": "2021-02-04T17:47:50.252878Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Coefficient:', model.coef_)\n",
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OLS\n",
    "\n",
    "### Using Python objects \n",
    "\n",
    "No intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:24:26.375912Z",
     "start_time": "2021-02-04T20:24:26.098532Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:24:30.344056Z",
     "start_time": "2021-02-04T20:24:30.336746Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tools import add_constant\n",
    "x_plus_intercept = add_constant(X)\n",
    "model = sm.OLS(Y, x_plus_intercept)\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the formula option\n",
    "\n",
    "No intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:25:16.816317Z",
     "start_time": "2021-02-04T20:25:16.790019Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([X,Y]).T\n",
    "df.columns = ['X', 'Y']\n",
    "formula = 'Y~X-1'\n",
    "model = sm.OLS.from_formula(formula, data=df)\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:25:53.423269Z",
     "start_time": "2021-02-04T20:25:53.408176Z"
    }
   },
   "outputs": [],
   "source": [
    "formula = 'Y~X+1'\n",
    "model = sm.OLS.from_formula(formula, data=df)\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Tables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:23:15.368962Z",
     "start_time": "2021-02-04T20:23:15.366828Z"
    }
   },
   "source": [
    "### Visualize simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_line(X, Y):\n",
    "    \"\"\"This function plots the best-fit line for a group of datapoints broken into\n",
    "    a set of independent variable values (X) and a set of dependent variable values (Y)\"\"\"\n",
    "    \n",
    "    # Create a linear regression model using \n",
    "    # either Sklearn or Statsmodels\n",
    "\n",
    "    # Collect beta_1 and beta_0\n",
    "    beta_1 = None\n",
    "    beta_0 = None\n",
    "    \n",
    "    # Plot the best fit line!\n",
    "    Xs = np.linspace(np.min(X), np.max(X), 100)\n",
    "    Ys = beta_1 * Xs + beta_0\n",
    "    plt.scatter(X, Y, label='datapoints', c='red')\n",
    "    plt.plot(Xs, Ys, 'k', label=f'y={round(beta_1, 2)}x+{round(beta_0, 2)}')\n",
    "    plt.legend(loc='upper left', framealpha=1)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T16:06:50.482481Z",
     "start_time": "2021-02-04T16:06:50.322519Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Using best_line\n",
    "best_line(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T16:11:55.166439Z",
     "start_time": "2021-02-04T16:11:54.989505Z"
    }
   },
   "outputs": [],
   "source": [
    "ans = sns.load_dataset('anscombe')\n",
    "sns.scatterplot(data=ans, x='x', y='y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T16:11:42.210312Z",
     "start_time": "2021-02-04T16:11:42.032763Z"
    }
   },
   "outputs": [],
   "source": [
    "best_line(ans.x, ans.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-fit line exists no matter what my data look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:59:58.686389Z",
     "start_time": "2021-02-04T15:59:58.524209Z"
    }
   },
   "outputs": [],
   "source": [
    "X_rand = stats.uniform.rvs(size=100)\n",
    "Y_rand = stats.uniform.rvs(size=100)\n",
    "plt.scatter(X_rand, Y_rand, c='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:59:59.266902Z",
     "start_time": "2021-02-04T15:59:59.110487Z"
    }
   },
   "outputs": [],
   "source": [
    "best_line(X_rand, Y_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    ">Code Simple Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/equation.png\" style=\"width:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_line(X, Y):\n",
    "    \"\"\"This function plots the best-fit line for a group of datapoints broken into\n",
    "    a set of independent variable values (X) and a set of dependent variable values (Y)\"\"\"\n",
    "   \n",
    "    # Calculate the means for both arrays\n",
    "\n",
    "    \n",
    "    # Calculate the difference between each observation\n",
    "    # and the means for both arrays\n",
    "\n",
    "    # Multiple the differences together\n",
    "\n",
    "    \n",
    "    # Sum the squared differences for each array\n",
    "    # and multiple them together.\n",
    "    # Take the square root of their product.\n",
    "\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "\n",
    "    \n",
    "    # Multiple the correlation coefficient\n",
    "    # by the standard deviation of y \n",
    "    # divided by the standard deviation of x\n",
    "\n",
    "    \n",
    "    # Multiply beta_1 and the mean of the X\n",
    "    # Subtract the product from the mean of Y\n",
    "\n",
    "    \n",
    "    # Plot the best fit line!\n",
    "    Xs = np.linspace(np.min(X), np.max(X), 100)\n",
    "    Ys = beta_1 * Xs + beta_0\n",
    "    plt.scatter(X, Y, label='datapoints', c='red')\n",
    "    plt.plot(Xs, Ys, 'k', label=f'y={round(beta_1, 2)}x+{round(beta_0, 2)}')\n",
    "    plt.legend(loc='upper left', framealpha=1)\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
