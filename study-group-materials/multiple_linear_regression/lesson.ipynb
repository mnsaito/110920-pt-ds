{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T19:30:39.166720Z", "start_time": "2021-02-09T19:30:39.163977Z"}}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from random import gauss\n", "from scipy import stats\n", "import seaborn as sns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Multiple Linear Regression\n", "\n", "The main idea here is pretty simple. Whereas, in simple linear regression we took our dependent variable to be a function only of a single independent variable, here we'll be taking the dependent variable to be a function of multiple independent variables.\n", "\n", "Our regression equation, then, instead of looking like $\\hat{y} = mx + b$, will now look like:\n", "\n", "$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_nx_n$.\n", "\n", "Remember that the hats ( $\\hat{}$ ) indicate parameters that are estimated.\n", "\n", "Is this still a best-fit *line*? Well, no. What does the graph of, say, z = x + y look like? [Here's](https://academo.org/demos/3d-surface-plotter/) a 3d-plotter. (Of course, once we get x's with subscripts beyond 2 it's going to be very hard to visualize. But in practice linear regressions can make use of dozens or even of hundreds of independent variables!)\n", "\n", "I want to focus here more on what coding a multiple regression looks like in Python. But you might be wondering: Is it possible to calculate the betas by hand?\n", "\n", "Yes! See [here](https://stattrek.com/multiple-regression/regression-coefficients.aspx) for a nice explanation and example.\n", "\n", "We'll focus more directly on matrix mathematics later in the course."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Confounding Variables\n", "\n", "Suppose I have a simple linear regression that models the growth of corn plants as a function of the temperature of the ambient air. And suppose there is a noticeable positive correlation between temperature and plant height."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:15:28.416667Z", "start_time": "2021-02-09T21:15:28.411629Z"}}, "outputs": [], "source": ["corn = pd.read_csv('data/corn.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:15:31.057037Z", "start_time": "2021-02-09T21:15:30.605323Z"}}, "outputs": [], "source": ["sns.lmplot(data=corn, x='temp', y='height')\n", "plt.xlabel('Temperature ($\\degree$ F)')\n", "plt.ylabel('Height (cm)')\n", "plt.title('Corn plant height as a function of temperature');"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["corn.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It seems that higher temperatures lead to taller corn plants. But it's hard to know for sure. One **confounding variable** might be *humidity*. If we haven't controlled for humidity, then it's difficult to draw conclusions.\n", "\n", "One solution is to use **both features** in a single model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:16:43.266816Z", "start_time": "2021-02-09T21:16:42.925776Z"}}, "outputs": [], "source": ["sns.lmplot(data=corn, x='humid', y='height')\n", "plt.xlabel('Humidity (%)')\n", "plt.ylabel('Height (cm)')\n", "plt.title('Corn plant height as a function of humidity');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Assumptions of Multiple Linear Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Multipe Linear Regession has the same assumptions as Simple Linear Regression **with one additional assumption.**\n", "<br>\n", "<br>\n", "<details><summary>Click here to review the assumptions of Simple Linear Regression</summary>\n", "    \n", "### 1. The relationship between target and predictor(s) is linear. (Of course!)**\n", "\n", "**How can I check for this?**\n", "- Build a scatterplot of y vs. various predictors.\n", "\n", "**What can I do if it looks like I'm violating this assumption?**\n", "- Consider log-scaling your data.\n", "- Consider a different type of model!\n", "\n", "### 2. The errors are mutually independent. (That is, there is no correlation between any two errors.)**\n", "\n", "**How can I check for this?**\n", "- Build an error plot, i.e. a plot of errors for a particular predictor (vs. the values of that predictor).\n", "\n", "**What can I do if it looks like I'm violating this assumption?**\n", "- Consider dropping extreme values.\n", "\n", "### 3. The errors are normally distributed. (That is, smaller errors are more probable than larger errors, according to the familiar bell curve.)**\n", "\n", "**How can I check for this?**\n", "- Check the Omnibus value (see below).\n", "- Check the Jarque-Bera value (see below).\n", "- Build a QQ-Plot.\n", "\n", "**What can I do if it looks like I'm violating this assumption?**\n", "- Consider log-scaling your data.\n", "\n", "### 4. The errors are homoskedastic. (That is, the errors have the same variance. The Greek word $\\sigma\\kappa\\epsilon\\delta\\acute{\\alpha}\\nu\\nu\\upsilon\\mu\\iota$ means \"to scatter\".)**\n", "\n", "**How can I check for this?**\n", "- Check the Durbin-Watson score (see below).\n", "- Conduct a Goldfeld-Quandt test.\n", "- Build an error plot, i.e. a plot of errors for a particular predictor (vs. the values of that predictor).\n", "\n", "**What can I do if it looks like I'm violating this assumption?**\n", "- Consider dropping extreme values.\n", "- Consider log-scaling your target.\n", "- Consider a different type of model!</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Multicollinearity\n", "\n", "Multicollinearity describes the correlation between distinct predictors. Why might high multicollinearity be a problem for interpreting a linear regression model?\n", "\n", "It's problematic for statistics in an inferential mode because, if $x_1$ and $x_2$ are highly correlated with $y$ but also *with each other*, then it will be very difficult to tease apart the effects of $x_1$ on $y$ and the effects of $x_2$ on $y$. If I really want to have a good sense of the effect of $x_1$ on $y$, then I'd like to vary $x_1$ while keeping the other features constant. But if $x_1$ is highly correlated with $x_2$ then this will be a practically impossible exercise!\n", "\n", "We will return to this topic again. For more, see [this post](https://towardsdatascience.com/https-towardsdatascience-com-multicollinearity-how-does-it-create-a-problem-72956a49058).\n", "\n", "### A further assumption for multiple linear regression is:\n", "\n", "**5. The predictors are independent.**\n", "\n", "**How can I check for this?**\n", "- Check the model Condition Number (see below).\n", "- Check the correlation values.\n", "- Compute Variance Inflation Factors ([VIFs](https://www.statsmodels.org/devel/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)).\n", "\n", "**What can I do if it looks like I'm violating this assumption?**\n", "\n", "- Consider dropping offending predictors.\n", "- Feature engineering\n", "- Scaling the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:25:38.558981Z", "start_time": "2021-02-09T21:25:38.364084Z"}}, "outputs": [], "source": ["corn.corr()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Dealing with Categorical Variables\n", "\n", "One issue we'd like to resolve is what to do with categorical variables, i.e. variables that represent categories rather than continua. In a Pandas DataFrame, these columns may well have strings or objects for values, but they need not. A certain heart-disease dataset from Kaggle, for example, has a target variable that takes values 0-4, each representing a different stage of heart disease.\n", "\n", "### Dummying\n", "\n", "One very effective way of dealing with categorical variables is to dummy them out. What this involves is making a new column for _each categorical value in the column we're dummying out_.\n", "\n", "These new columns will be filled only with 0's and 1's, a 1 representing the presence of the relevant categorical value.\n", "\n", "Let's look at a simple example:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:27:33.118835Z", "start_time": "2021-02-09T21:27:33.109361Z"}}, "outputs": [], "source": ["comma_use = pd.read_csv('data/comma-survey.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For more on this dataset see [here](https://fivethirtyeight.com/features/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:27:34.596787Z", "start_time": "2021-02-09T21:27:34.581024Z"}}, "outputs": [], "source": ["comma_use.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:28:33.257367Z", "start_time": "2021-02-09T21:28:33.251785Z"}}, "outputs": [], "source": ["comma_use['In your opinion, which sentence is more gramatically correct?'].value_counts()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:29:03.529084Z", "start_time": "2021-02-09T21:29:03.525244Z"}}, "outputs": [], "source": ["comma_use.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:29:07.290829Z", "start_time": "2021-02-09T21:29:07.285150Z"}, "scrolled": false}, "outputs": [], "source": ["comma_use.dropna(inplace=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:29:08.006071Z", "start_time": "2021-02-09T21:29:08.002436Z"}}, "outputs": [], "source": ["comma_use.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:31:06.089555Z", "start_time": "2021-02-09T21:31:06.078151Z"}}, "outputs": [], "source": ["# Let's try using sklearn's OneHotEncoder to create our dummy columns:\n", "from sklearn.preprocessing import OneHotEncoder\n", "\n", "ohe = OneHotEncoder(drop='first')\n", "comma_trans = ohe.fit_transform(comma_use.drop('RespondentID', axis=1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:31:13.641028Z", "start_time": "2021-02-09T21:31:13.637277Z"}}, "outputs": [], "source": ["comma_trans.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#pd.get_dummies(comma_use)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Could we have used ```pd.get_dummies()``` instead?\n", "\n", "Well, yes. And in fact ```get_dummies()``` is in some ways easier; for one thing, it's built right into Pandas. But there are drawbacks with it as well. See the *bottom* of [this link](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons) for a good explanation.\n", "\n", "So what did the encoder do?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:32:29.233193Z", "start_time": "2021-02-09T21:32:29.229201Z"}}, "outputs": [], "source": ["comma_trans"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:32:55.145851Z", "start_time": "2021-02-09T21:32:55.141050Z"}}, "outputs": [], "source": ["comma_trans.todense()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:33:22.699145Z", "start_time": "2021-02-09T21:33:22.695157Z"}}, "outputs": [], "source": ["ohe.get_feature_names()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:39:59.299285Z", "start_time": "2021-02-09T21:39:59.274142Z"}}, "outputs": [], "source": ["df = pd.DataFrame(comma_trans.todense(), columns=ohe.get_feature_names())\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Multiple Regression in StatsModels\n", "\n", "Statsmodels offers a highly descriptive report of the fit of a regression model. Let's generate a simple regression and then analyze the report!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:40:12.986927Z", "start_time": "2021-02-09T21:40:12.984484Z"}}, "outputs": [], "source": ["import statsmodels.api as sm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First let's try data that fit a straight line perfectly:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:40:30.682366Z", "start_time": "2021-02-09T21:40:30.668117Z"}}, "outputs": [], "source": ["x = np.arange(20)\n", "y = 3*x + 1         # Note that we can do this only because x is a NumPy array!\n", "\n", "sm.OLS(y, sm.add_constant(x)).fit().summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\rightarrow$Now let's add a little noise:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:43:28.883651Z", "start_time": "2021-02-09T21:43:28.880295Z"}}, "outputs": [], "source": ["x = np.arange(20)\n", "y = np.array([3*pt + 1 + gauss(mu=0, sigma=5) for pt in x])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:43:34.508795Z", "start_time": "2021-02-09T21:43:34.500932Z"}}, "outputs": [], "source": ["df2 = pd.DataFrame(columns=['x', 'y'])\n", "\n", "df2['x'] = x\n", "df2['y'] = y"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:43:47.713204Z", "start_time": "2021-02-09T21:43:47.703045Z"}}, "outputs": [], "source": ["model = sm.formula.ols(formula='y~x', data=df2).fit()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:43:48.603676Z", "start_time": "2021-02-09T21:43:48.590066Z"}}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Please note the difference between `sm.OLS()` and `sm.formula.ols()`!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:44:58.413293Z", "start_time": "2021-02-09T21:44:57.594818Z"}}, "outputs": [], "source": ["sm.graphics.plot_regress_exog(model, 'x', fig=plt.figure(figsize=(12, 8)));"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Fitted Model Attributes and Methods\n", "\n", "The fitted model has [many](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html) attributes and methods. I'll look at a couple here."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:50:33.464950Z", "start_time": "2021-02-09T21:50:33.460527Z"}}, "outputs": [], "source": ["model.tvalues"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:50:34.687706Z", "start_time": "2021-02-09T21:50:34.682390Z"}, "scrolled": true}, "outputs": [], "source": ["model.pvalues"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:50:43.560983Z", "start_time": "2021-02-09T21:50:43.557593Z"}}, "outputs": [], "source": ["model.mse_total"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `.summary()` method contains lots of helpful information about the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T21:50:51.649263Z", "start_time": "2021-02-09T21:50:51.637034Z"}, "scrolled": false}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What are all these statistics!? Let's say a word about them."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Coefficient of Determination\n", "\n", "Very often a data scientist will calculate $R^2$, the *coefficient of determination*, as a measure of how well the model fits the data.\n", "\n", "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n", "\n", "The actual calculation of $R^2$ is: <br/> $\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$.\n", "\n", "$R^2$ is a measure of how much variation in the dependent variable your model explains.\n", "\n", "### Adjusted $R^2$\n", "\n", "There are some theoretical [objections](https://data.library.virginia.edu/is-r-squared-useless/) to using $R^2$ as an evaluator of a regression model.\n", "\n", "One objection is that, if we add another predictor to our model, $R^2$ can only *increase*! (It could hardly be that with more features I'd be able to account for *less* of the variation in the dependent variable than I could with the smaller set of features.)\n", "\n", "One improvement is **adjusted $R^2$**: <br/> $\\Large R^2_{adj.}\\equiv 1 - \\frac{(1 - R^2)(n - 1)}{n - m - 1}$, where:\n", "\n", "- n is the number of data points; and\n", "- m is the number of predictors.\n", "\n", "This can be a better indicator of the quality of a regression model. For more, see [here](https://www.statisticshowto.datasciencecentral.com/adjusted-r2/)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that $R^2$ *can* be negative!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T14:41:43.662899Z", "start_time": "2021-02-09T14:41:43.613487Z"}}, "outputs": [], "source": ["from sklearn.datasets import make_regression\n", "from sklearn.metrics import r2_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T14:41:44.176217Z", "start_time": "2021-02-09T14:41:44.169209Z"}}, "outputs": [], "source": ["X, y = make_regression()\n", "\n", "bad_pred = np.mean(y) * np.ones(len(y))\n", "worse_pred = (np.mean(y) - 100) * np.ones(len(y))\n", "\n", "print(r2_score(y, bad_pred))\n", "print(r2_score(y, worse_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Other Regression Statistics\n", "\n", "What else do we have in this report?\n", "\n", "- **F-statistic**: The F-test measures the significance of your model relative to a model in which all coefficients are 0, i.e. relative to a model that says there is no correlation whatever between the predictors and the target. <br/><br/>\n", "- **Log-Likelihood**: The probability in question is the probability of seeing these data points, *given* the model parameter values. The higher this is, the more our data conform to our model and so the better our fit. AIC and BIC are related to the log-likelihood; we'll talk about those later. <br/><br/>\n", "- **coef**: These are the betas as calculated by the least-squares regression. We also have p-values and 95%-confidence intervals. <br/><br/>\n", "- **Omnibus**: This is a test for error normality. The probability is the chance that the errors are normally distributed. <br/><br/>\n", "- **Durbin-Watson**: This is a test for error homoskedasticity. We're looking for values between ~1.5 and ~2.5. <br/><br/>\n", "- **Jarque-Bera**: This is another test for error normality. <br/><br/>\n", "- **Cond. No.**: The condition number tests for independence of the predictors. Lower scores are better. When the predictors are *not* independent, we can run into problems of multicollinearity. For more on the condition number, see [here](https://stats.stackexchange.com/questions/168259/how-do-you-interpret-the-condition-number-of-a-correlation-matrix).\n", "\n", "**Many good regression diagnostics are available in** [statsmodels](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html). For more on statsmodels regression statistics, see [here](https://www.accelebrate.com/blog/interpreting-results-from-linear-regression-is-the-data-appropriate)."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Drk8n9Fugd9R"}, "source": ["# Interpreting a linear regression model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T20:01:09.759439Z", "start_time": "2021-02-09T20:01:09.752739Z"}, "colab": {}, "colab_type": "code", "id": "VjPLPhj1gd9W"}, "outputs": [], "source": ["df = pd.read_csv(\"data/nhanes.csv\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "bn3-A4H6gd9g"}, "source": ["### Simple Linear Regression\n", "\n", "In the cell below, we create a linear regression model that calculates blood presure as a function of age. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T20:01:11.713138Z", "start_time": "2021-02-09T20:01:11.687112Z"}, "colab": {}, "colab_type": "code", "id": "XfDp1slKgd9h", "outputId": "287ba8c7-859b-4e85-b354-506c278039f3"}, "outputs": [], "source": ["model = sm.OLS.from_formula(\"blood_pressure ~ age\", data=df)\n", "result = model.fit()\n", "result.summary()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "rs88E0iMgd9n"}, "source": ["### R-squared and correlation\n", "\n", "Looking at our R-Squares, we can say that this model, or the age column, explains 21%\n", "of the variance in blood pressure."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "1AmPwraTgd9s"}, "source": ["### Adding a categorical\n", "\n", "Next, let's add gender to our model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T20:01:23.621139Z", "start_time": "2021-02-09T20:01:23.615786Z"}, "colab": {}, "colab_type": "code", "id": "ztS5b0ptgd9t"}, "outputs": [], "source": ["transform = lambda x: \"male\" if x == 1 else \"female\"\n", "df['gender_cat'] = df.gender.apply(transform)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T20:01:35.573984Z", "start_time": "2021-02-09T20:01:35.528930Z"}, "colab": {}, "colab_type": "code", "id": "1VSTpMLWgd9u", "outputId": "8ad2ccc3-cb12-48f0-d4da-2aeddbf638b7", "scrolled": false}, "outputs": [], "source": ["model = sm.OLS.from_formula(\"blood_pressure ~ age + gender_cat\", data=df)\n", "result = model.fit()\n", "result.summary()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "xfPLqR0agd9y"}, "source": ["\n", "------\n", "When interpreting coefficients for multiple predictors, it is important to interpret them with all other coefficients held constant. \n", "\n", "So for example, the the above model finds that if two people share the same gender and have an age difference of one year, their blood pressure will differ on average by 0.47. Following this same logic, how can we interpret the coefficient for gender?\n", "\n", "----"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T20:08:32.898103Z", "start_time": "2021-02-09T20:08:32.889179Z"}, "colab": {}, "colab_type": "code", "id": "pQXMD09lgd9z", "outputId": "a7d887fa-c813-4d90-8c8b-0bc3df23c289"}, "outputs": [], "source": ["df[[\"age\", \"gender\"]].corr()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "k0Tl6mtngd92"}, "source": ["### Let's add a third predictor.\n", "\n", "In the cell below we add, body mass index ([BMI](https://en.wikipedia.org/wiki/Body_mass_index)) to our model for blood pressure. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T20:10:16.927563Z", "start_time": "2021-02-09T20:10:16.882241Z"}, "colab": {}, "colab_type": "code", "id": "mxFsbNOFgd93", "outputId": "d6324212-40a1-43ac-b872-c5c037d4dcbf"}, "outputs": [], "source": ["model = sm.OLS.from_formula(\"blood_pressure ~ age + bmi + gender_cat\", data=df)\n", "result = model.fit()\n", "result.summary()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "NmdQDd1ugd96"}, "source": ["How would we interpret this model?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-02-09T19:25:34.421760Z", "start_time": "2021-02-09T19:25:34.412482Z"}, "colab": {}, "colab_type": "code", "id": "BweSe7ozgd97", "outputId": "fc64c74e-d096-4577-c666-9ca1d0048a8c"}, "outputs": [], "source": ["df[[\"age\", \"gender\", \"bmi\"]].corr()"]}], "metadata": {"colab": {"collapsed_sections": [], "name": "Linear Regression NHANES Walkthrough.ipynb", "provenance": [], "version": "0.3.2"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}